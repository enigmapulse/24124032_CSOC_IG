{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35013857",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45da16f",
   "metadata": {},
   "source": [
    "> In this notebook, we will be implementing policy iteration and value iteration, which are both a dynamic programming algorthm used to find the optimal policy for an agent interacting with an environment which satisfies the Markov property. These lay the foundation of reinforcement learning. We will be testing our implementation on FrozenLake environment and some custom versions we made. Finally we will compare and try to draw conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b767670",
   "metadata": {},
   "source": [
    "## Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "2b1d2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d056b3",
   "metadata": {},
   "source": [
    "## Custom Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c43f0ae",
   "metadata": {},
   "source": [
    "Here we are creating a new class for our custom environment with a 5*5 custom map and similar methods to the standard environments in the gymnasium package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc70610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFrozenLakeEnv(gym.Env):\n",
    "    def __init__(self, slippery=False):\n",
    "        self.size = 5\n",
    "        self.n_states = self.size * self.size\n",
    "        self.n_actions = 4\n",
    "        self.slippery = slippery\n",
    "        \n",
    "        # Define custom 5x5 map\n",
    "        self.desc = np.array([\n",
    "            ['S', 'F', 'F', 'F', 'F'],\n",
    "            ['F', 'H', 'F', 'H', 'F'],\n",
    "            ['F', 'F', 'F', 'F', 'H'],\n",
    "            ['H', 'F', 'F', 'F', 'F'],\n",
    "            ['F', 'F', 'H', 'F', 'G']\n",
    "        ], dtype='<U1')\n",
    "        \n",
    "        # Calculate positions\n",
    "        self.start_pos = None\n",
    "        self.goal_pos = []\n",
    "        self.hole_pos = []\n",
    "        for row in range(self.size):  ## idea was that there may be many goals but not doing that anymore\n",
    "            for col in range(self.size):\n",
    "                if self.desc[row, col] == 'S':\n",
    "                    self.start_pos = (row, col)\n",
    "                elif self.desc[row, col] == 'G':\n",
    "                    self.goal_pos.append((row, col))\n",
    "                elif self.desc[row, col] == 'H':\n",
    "                    self.hole_pos.append((row, col))\n",
    "        \n",
    "        # first is row and second is coulumn\n",
    "        self.actions = {\n",
    "            0: (0, -1),    # Left\n",
    "            1: (1, 0),     # Down\n",
    "            2: (0, 1),     # Right\n",
    "            3: (-1, 0)     # Up\n",
    "        }\n",
    "        \n",
    "        # Build transition model\n",
    "        self.P = self._make_transition_model()\n",
    "        \n",
    "        # Define spaces and state\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        self.observation_space = spaces.Discrete(self.n_states)\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self, seed=None, **kwargs):\n",
    "        super().reset(seed=seed, **kwargs)\n",
    "        self.state = self.pos_to_state(self.start_pos) #what;s the satte of the start position\n",
    "        return self.state, {} # we ain't sending any info\n",
    "\n",
    "    def step(self, action):\n",
    "        current_pos = self.state_to_pos(self.state) # we are only storing state not position so this roundabout\n",
    "        row, col = current_pos\n",
    "        \n",
    "        # if it is slippery then some stochastic flavour\n",
    "        if self.slippery:\n",
    "            action = self.np_random.choice([action, (action + 1) % 4, (action - 1) % 4])\n",
    "        \n",
    "\n",
    "        # calculus :)\n",
    "        dr, dc = self.actions[action]\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "        \n",
    "        # Ensure within bounds\n",
    "        new_row = np.clip(new_row, 0, self.size - 1)\n",
    "        new_col = np.clip(new_col, 0, self.size - 1)\n",
    "        new_pos = (new_row, new_col)\n",
    "        new_state = self.pos_to_state(new_pos)\n",
    "        \n",
    "        # Check for hole or goal\n",
    "        terminated = False\n",
    "        reward = 0.0\n",
    "        if new_pos in self.hole_pos:\n",
    "            terminated = True\n",
    "        elif new_pos in self.goal_pos:\n",
    "            terminated = True\n",
    "            reward = 1.0\n",
    "        \n",
    "        self.state = new_state #change state\n",
    "        return new_state, reward, terminated, False, {}\n",
    "\n",
    "    \n",
    "\n",
    "    def _make_transition_model(self):\n",
    "        P = {s: {a: [] for a in range(self.n_actions)} for s in range(self.n_states)} #initializing P[s][a] to an empty list\n",
    "        for s in range(self.n_states):\n",
    "            row, col = self.state_to_pos(s)\n",
    "            # Check if current state is terminal\n",
    "            current_pos = (row, col)\n",
    "            if current_pos in self.hole_pos or current_pos in self.goal_pos:\n",
    "                for a in range(self.n_actions):\n",
    "                    # Terminal state: stay indefinitely with 0 reward\n",
    "                    P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                continue  # Skip normal transition logic\n",
    "\n",
    "            # Existing transition logic for non-terminal states\n",
    "            for a in range(self.n_actions):\n",
    "                outcomes = []\n",
    "                actions_to_consider = ([a] if not self.slippery else [a, (a+1)%4, (a-1)%4])\n",
    "                prob = 1.0 if not self.slippery else 1/3 # stochastic case\n",
    "                for a2 in actions_to_consider:\n",
    "                    #compute new state\n",
    "                    dr, dc = self.actions[a2]\n",
    "                    nr = np.clip(row + dr, 0, self.size - 1)\n",
    "                    nc = np.clip(col + dc, 0, self.size - 1)\n",
    "                    new_pos = (nr, nc)\n",
    "                    ns = self.pos_to_state(new_pos)\n",
    "                    #check and assign prob accordingly\n",
    "                    done = new_pos in self.hole_pos or new_pos in self.goal_pos\n",
    "                    reward = 1.0 if new_pos in self.goal_pos else 0.0\n",
    "                    outcomes.append((prob, ns, reward, done))\n",
    "                P[s][a] = outcomes\n",
    "        return P\n",
    "\n",
    "    # easy peasy\n",
    "    def pos_to_state(self, pos):\n",
    "        row, col = pos\n",
    "        return row * self.size + col\n",
    "\n",
    "    def state_to_pos(self, state):\n",
    "        row = state // self.size\n",
    "        col = state % self.size\n",
    "        return (row, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b8e687",
   "metadata": {},
   "source": [
    "This is another custom environment similar to FrozenLake with a 4*4 size except with a twist. The agent will be rewarded only when it collects key along its path to reach the goal. This forces the agent to adopt a particular route. Kind of like travelling in a traffic where you are required to visit a stop (say a petrol pump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b9c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedFrozenLakeEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, slippery=False):\n",
    "        self.size = 4\n",
    "        self.n_states_base = self.size * self.size\n",
    "        self.n_states = self.n_states_base * 2  # Double for key status\n",
    "        self.n_actions = 4\n",
    "        self.slippery = slippery\n",
    "        \n",
    "        # Define 4x4 map with key\n",
    "        self.desc = np.array([\n",
    "            ['S', 'F', 'F', 'F'],\n",
    "            ['F', 'H', 'F', 'K'],\n",
    "            ['F', 'F', 'F', 'F'],\n",
    "            ['H', 'F', 'F', 'G']\n",
    "        ], dtype='<U1')\n",
    "        \n",
    "        # Identify key, start, goal, and hole positions\n",
    "        self.start_pos = None\n",
    "        self.goal_pos = []\n",
    "        self.hole_pos = []\n",
    "        self.key_pos = None\n",
    "        for row in range(self.size): # same idea as for the above class\n",
    "            for col in range(self.size):\n",
    "                if self.desc[row, col] == 'S':\n",
    "                    self.start_pos = (row, col)\n",
    "                elif self.desc[row, col] == 'G':\n",
    "                    self.goal_pos.append((row, col))\n",
    "                elif self.desc[row, col] == 'H':\n",
    "                    self.hole_pos.append((row, col))\n",
    "                elif self.desc[row, col] == 'K':\n",
    "                    self.key_pos = (row, col)\n",
    "        \n",
    "        self.actions = {\n",
    "            0: (0, -1),   # Left\n",
    "            1: (1, 0),    # Down\n",
    "            2: (0, 1),    # Right\n",
    "            3: (-1, 0)    # Up\n",
    "        }\n",
    "\n",
    "        self.P = self._make_transition_model()\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        self.observation_space = spaces.Discrete(self.n_states)\n",
    "        # Initialize state\n",
    "        self.state = None\n",
    "        self.has_key = None\n",
    "\n",
    "    def reset(self, seed=None, **kwargs):\n",
    "        super().reset(seed=seed, **kwargs)\n",
    "        self.has_key = False\n",
    "        self.state = self.pos_to_state(self.start_pos)\n",
    "        return self.get_full_state(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        current_pos = self.state_to_pos(self.state)\n",
    "        row, col = current_pos\n",
    "        \n",
    "        # act \n",
    "        if self.slippery:\n",
    "            action = self.np_random.choice([action, (action + 1) % 4, (action - 1) % 4])\n",
    "        \n",
    "        dr, dc = self.actions[action]\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "        \n",
    "        # Ensure within bounds\n",
    "        new_row = np.clip(new_row, 0, self.size - 1)\n",
    "        new_col = np.clip(new_col, 0, self.size - 1)\n",
    "        new_pos = (new_row, new_col)\n",
    "        new_state = self.pos_to_state(new_pos)\n",
    "        \n",
    "        # Check if key is collected\n",
    "        if new_pos == self.key_pos:\n",
    "            self.has_key = True\n",
    "        \n",
    "        # Check for hole or goal\n",
    "        terminated = False\n",
    "        reward = 0.0\n",
    "        if new_pos in self.hole_pos:\n",
    "            terminated = True\n",
    "        elif new_pos in self.goal_pos and self.has_key:\n",
    "            terminated = True\n",
    "            reward = 1.0\n",
    "        \n",
    "        self.state = new_state\n",
    "        full_state = self.get_full_state()\n",
    "        return full_state, reward, terminated, False, {}\n",
    "\n",
    "    def _make_transition_model(self):\n",
    "        n_base = self.n_states_base  # 16 for 4x4 grid\n",
    "        P = {s: {a: [] for a in range(self.n_actions)} for s in range(self.n_states)} #initialize P[s][a] to an empty list\n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            base_state = s % n_base\n",
    "            has_key = (s >= n_base)\n",
    "            row, col = self.state_to_pos(base_state)\n",
    "            current_pos = (row, col)\n",
    "\n",
    "            # Terminal states: hole OR goal with key\n",
    "            if current_pos in self.hole_pos or (current_pos in self.goal_pos and has_key):\n",
    "                for a in range(self.n_actions):\n",
    "                    P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                continue #skipping normal logic for now\n",
    "\n",
    "            for a in range(self.n_actions):\n",
    "                outcomes = []\n",
    "                actions_to_consider = [a] if not self.slippery else [a, (a+1)%4, (a-1)%4]\n",
    "                prob = 1.0 if not self.slippery else 1.0/len(actions_to_consider)\n",
    "                \n",
    "                for a2 in actions_to_consider:\n",
    "                    #compute new state\n",
    "                    dr, dc = self.actions[a2]\n",
    "                    nr = np.clip(row + dr, 0, self.size-1)\n",
    "                    nc = np.clip(col + dc, 0, self.size-1)\n",
    "                    new_pos = (nr, nc)\n",
    "                    new_base_state = self.pos_to_state(new_pos)\n",
    "                    \n",
    "                    # Update key status\n",
    "                    new_has_key = has_key\n",
    "                    if not has_key and new_pos == self.key_pos:\n",
    "                        new_has_key = True\n",
    "                    \n",
    "                    next_state = new_base_state + (n_base * int(new_has_key))\n",
    "                    \n",
    "                    # Check terminal conditions\n",
    "                    terminated = False\n",
    "                    reward = 0.0\n",
    "                    if new_pos in self.hole_pos:\n",
    "                        terminated = True\n",
    "                    elif new_pos in self.goal_pos and new_has_key:\n",
    "                        terminated = True\n",
    "                        reward = 1.0\n",
    "                    \n",
    "                    outcomes.append((prob, next_state, reward, terminated))\n",
    "                \n",
    "                P[s][a] = outcomes\n",
    "        \n",
    "        return P\n",
    "\n",
    "    #trivial stuff\n",
    "    def pos_to_state(self, pos):\n",
    "        row, col = pos\n",
    "        return row * self.size + col\n",
    "\n",
    "    def state_to_pos(self, state):\n",
    "        row = state // self.size\n",
    "        col = state % self.size\n",
    "        return (row, col)\n",
    "\n",
    "    def get_full_state(self):\n",
    "        return self.state + (self.n_states_base * int(self.has_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e294749",
   "metadata": {},
   "source": [
    "### Registering these custom environmnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "d3cd5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register environments\n",
    "gym.register(\n",
    "    id=\"CustomFrozenLake-v1\",\n",
    "    entry_point=CustomFrozenLakeEnv,\n",
    "    kwargs={'slippery': False},\n",
    ")\n",
    "\n",
    "gym.register(\n",
    "    id=\"ExpandedFrozenLake-v1\",\n",
    "    entry_point=ExpandedFrozenLakeEnv,\n",
    "    kwargs={'slippery': False},\n",
    ")\n",
    "\n",
    "gym.register(\n",
    "    id=\"CustomFrozenLake-v1-slip\",\n",
    "    entry_point=CustomFrozenLakeEnv,\n",
    "    kwargs={'slippery': True},\n",
    ")\n",
    "\n",
    "gym.register(\n",
    "    id=\"ExpandedFrozenLake-v1-slip\",\n",
    "    entry_point=ExpandedFrozenLakeEnv,\n",
    "    kwargs={'slippery': True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c606f4e",
   "metadata": {},
   "source": [
    "We will iterate over these environment ids to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "0a552f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env ids to test\n",
    "env_ids = ['FrozenLake-v1','CustomFrozenLake-v1','ExpandedFrozenLake-v1','CustomFrozenLake-v1-slip','ExpandedFrozenLake-v1-slip']  # frozenlake-v1 is for the default "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19bffd4",
   "metadata": {},
   "source": [
    "## Dynamic Programming Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e5d662",
   "metadata": {},
   "source": [
    "### Policy iteration\n",
    "\n",
    "We will start off wth a random policy and evaluate the value function for all states according to this policy. This is known as Policy evaluation.\n",
    "\n",
    "Next we will improve our policy through ${Q}$ functions. We will check if performing action ${a}$ and then following policy ${\\pi}$  gives us a better value for that particular state than following ${\\pi}$ from beginning.\n",
    " If that is so, then by policy improvement theorem, it is always better to choose this action ${a}$ instead of following ${\\pi}$ when we are at that state.\n",
    " \n",
    "This way we frame our new policy for all the states and then again evaluate the value function. This process is repeated again and again, and it finally converges when we reach the optimal value functon and optimal policy.\n",
    "\n",
    "We can ensure that we have reached the optimal policy, provided there is no change in the value of all the states. Realistically though, we check if values have reached a certain tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0931a5",
   "metadata": {},
   "source": [
    "#### Policy Evaluation\n",
    "\n",
    "Below is the update equation for the values of our states which we need to implement in code. \n",
    "\n",
    "We will be going with the Gauss-Siedel updates which means that our list of values of each state will be updated in-place. That is instead of maintaining two lists, one for old and other for new, we will mantain a single list and use whatever values available to make the updates. This converges usually faster and consumes lesser space. \n",
    "\n",
    "Since we need to update each state, we will first run an outer loop on states. Then we see that the outer summation is over actions, so the next inner loop is over the set of all actions. The final summation is over the nest state and reward which we unpack from  env[s][a] and conclude our final loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98123792",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\Large{\\begin{aligned}\n",
    "v_{k+1}(s) &\\doteq \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma v_k(S_{t+1}) \\mid S_t = s \\right] \\\\\n",
    "          &= \\sum_a \\pi(a \\mid s) \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma v_k(s') \\right]\n",
    "\\end{aligned}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "753f1eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def policy_evaluation(env, policy, nStates, nActions, discount=0.99, tolerance=1e-6):\n",
    "\n",
    "    '''\n",
    "    ------------------------------------------------------------------------------\n",
    "    This function would return the value function for all states given \n",
    "    a policy and a model\n",
    "\n",
    "    nStates is number of states\n",
    "    nActions is number of actions\n",
    "\n",
    "    policy[s] = give the probability distribution over all actions\n",
    "\n",
    "    env[s][a] = is a dictionary of the form (prob, next_state, reward, done)\n",
    "    means if you perform action a in state s ten these are the probabilities\n",
    "    of moving to these states with this reward. done marks the end of the episode.\n",
    "    \n",
    "    tolerance would be used to determine when the value functions have converged.\n",
    "    -------------------------------------------------------------------------------\n",
    "    '''\n",
    "\n",
    "    values = np.zeros(nStates) # initialize all the values as zero\n",
    "    # side note: we can initialize them as we wish except that the\n",
    "    # all the terminal states must be initialized zero\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(nStates):\n",
    "            new_value_s = 0 #temporary var to store the result of computation since we are using a single array\n",
    "            for action, action_prob in enumerate(policy[state]): # enumerate will return an index which we store in action variable\n",
    "                for prob, nextState, reward, done in env[state][action]:\n",
    "                    new_value_s += action_prob * prob * (reward + discount*values[nextState]) #update equation\n",
    "            delta = max(delta, abs(new_value_s - values[state])) # the max difference between the new and old values\n",
    "            values[state] = new_value_s\n",
    "        if (delta < tolerance):\n",
    "            break\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a1d3f",
   "metadata": {},
   "source": [
    "#### Policy Improvement\n",
    "\n",
    "We want to change our policy so that when we encounter the state $s$ we always choose the action which maximizes the $\\Large{q_{\\pi}}$ function. \n",
    "\n",
    "Since we need to change the policy for each action the outermost loop is over the set of states. Then we need to find the maximum over all action for which we create a numpy array and then loop over te set of actions. Finally the last loop is over the next state and reward which we again unpack from the environment. In the end, we return this new policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724a9f7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large{\\begin{aligned}\n",
    "\\pi'(s) &\\doteq \\arg\\max_a q_\\pi(s, a) \\\\\n",
    "       &= \\arg\\max_a \\mathbb{E} \\left[ R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s, A_t = a \\right] \\\\\n",
    "       &= \\arg\\max_a \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]\n",
    "\\end{aligned}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "844f4bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, nStates, nActions, values, discount=0.99):\n",
    "    '''\n",
    "    --------------------------------------------------------------------------------------\n",
    "    This function would return a new policy given the value function and the environment\n",
    "\n",
    "    nStates is number of states\n",
    "    nActions is number of actions\n",
    "\n",
    "    env[s][a] = is a dictionary of the form (prob, next_state, reward, done)\n",
    "    means if you perform action a in state s ten these are the probabilities\n",
    "    of moving to these states with this reward. done marks the end of the episode.\n",
    "    ---------------------------------------------------------------------------------------\n",
    "    '''\n",
    "\n",
    "    policy = np.zeros([nStates, nActions]) # policy[s] would give probability distribution over all actions\n",
    "\n",
    "    for state in range(nStates):\n",
    "        q_states = np.zeros(nActions) #we will store all the q function list for each action here\n",
    "        for action in range(nActions):\n",
    "            for prob, state_next, reward, done in env[state][action]:\n",
    "                q_states[action] += prob * (reward + discount * values[state_next])\n",
    "        best_action = np.argmax(q_states)\n",
    "        policy[state, best_action] = 1.0\n",
    "\n",
    "    return policy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7f806",
   "metadata": {},
   "source": [
    "#### Policy Iteration (Implementation)\n",
    "\n",
    "We will finally be interleaving both the evaluation and the improvement of our policy in this final iteration and checking if we have reached the optimal condition by seeing if the new policy we are getting through policy improvement is the same as our old policy, meaning there is no further improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "d594882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, nStates, nActions, discount=0.99, max_iter = 1000):\n",
    "    policy = np.ones([nStates, nActions]) / nActions # initialize uniform random policy\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        values = policy_evaluation(env, policy, nStates, nActions, discount)\n",
    "        new_policy = policy_improvement(env, nStates, nActions, values, discount)\n",
    "\n",
    "        if np.all(new_policy == policy):\n",
    "            break\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy, values, iter+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e4842",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "Value iteration is another way of finding the optimal policy in the realm of dynamic programming. What we do here is choose the best possible value possible for that state by iterating over all possible actions. This way the value function converges to the optimal value function.  \n",
    "\n",
    "Then we extract the optimal policy from this optimal value function by iterating over all actions for all states and seeing which one maximizes the function. We then assign $\\Large{\\pi(s) = a}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c301fcd",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\Large{\\begin{aligned}\n",
    "v_{k+1}(s) &= \\max_a \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma v_k(s') \\right]\n",
    "\\end{aligned}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "6f577943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, nStates, nActions, discount=0.99, tolerance=1e-6):\n",
    "    '''\n",
    "    --------------------------------------------------------------------------------------\n",
    "    This function would return the most optimal policy and the value function given an \n",
    "    environment\n",
    "\n",
    "    nStates is number of states\n",
    "    nActions is number of actions\n",
    "\n",
    "    env[s][a] = is a dictionary of the form (prob, next_state, reward, done)\n",
    "    means if you perform action a in state s ten these are the probabilities\n",
    "    of moving to these states with this reward. done marks the end of the episode.\n",
    "    ---------------------------------------------------------------------------------------\n",
    "    '''\n",
    "\n",
    "    values = np.zeros(nStates)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(nStates):\n",
    "            q_states = np.zeros(nActions) #store all the q function values for all possible action\n",
    "            for action in range(nActions):\n",
    "                for prob, nextState, reward, done in env[state][action]:\n",
    "                    q_states[action] += prob * (reward + discount * values[nextState])\n",
    "            max_q = max(q_states)\n",
    "            delta = max(delta, abs(max_q - values[state]))\n",
    "            values[state] = max_q  # choose the best one and save it\n",
    "        if (delta < tolerance):\n",
    "            break\n",
    "\n",
    "    # extracting optimal policy from the optimal value function\n",
    "    policy = np.zeros([nStates, nActions])\n",
    "    for state in range(nStates):\n",
    "        q_states = np.zeros(nActions)\n",
    "        for action in range(nActions):\n",
    "            for prob, nextState, reward, done in env[state][action]:\n",
    "                q_states[action] += prob * (reward + discount * values[nextState]) \n",
    "        best_action = np.argmax(q_states) #choose the action maximizing the q function \n",
    "        policy[state, best_action] = 1.0\n",
    "    return policy, values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d2a26",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "Having made our optimal policy through policy and value iteration methods, we would like to test our policy in the environment to see how it performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "a14f2dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, nEpisodes=1000, max_steps=100):\n",
    "    '''\n",
    "    --------------------------------------------------------------------------------------\n",
    "    This function would return the mean reward and the mean number of steps taken over\n",
    "    all the episodes\n",
    "\n",
    "    nEpisodes is number of episodes\n",
    "    max_steps is the max number of steps to be taken in the environment\n",
    "\n",
    "    env[s][a] = is a dictionary of the form (prob, next_state, reward, done)\n",
    "    means if you perform action a in state s ten these are the probabilities\n",
    "    of moving to these states with this reward. done marks the end of the episode.\n",
    "\n",
    "    policy[s] give the probability distribution over all actions\n",
    "    ---------------------------------------------------------------------------------------\n",
    "    '''\n",
    "    rewards = []\n",
    "    lengths = []\n",
    "    for _ in range(nEpisodes):\n",
    "        obs, _ = env.reset() #first observation\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        steps = 0\n",
    "        while not done and steps < max_steps: #till either goal is reached or max_steps are taken\n",
    "            action = np.argmax(policy[obs])\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            ep_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "        lengths.append(steps)\n",
    "\n",
    "    return np.mean(rewards), np.mean(lengths)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce58fd",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We will now test our algorithm in three environment of FrozenLake and see the results of our DP algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "72ba8517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CustomFrozenLake-v1': {'PI': {'iters': 3,\n",
      "                                'length': np.float64(8.0),\n",
      "                                'reward': np.float64(1.0),\n",
      "                                'time': 0.0032219886779785156},\n",
      "                         'VI': {'length': np.float64(8.0),\n",
      "                                'reward': np.float64(1.0),\n",
      "                                'time': 0.0006968975067138672}},\n",
      " 'CustomFrozenLake-v1-slip': {'PI': {'iters': 3,\n",
      "                                     'length': np.float64(53.676),\n",
      "                                     'reward': np.float64(0.576),\n",
      "                                     'time': 0.046004295349121094},\n",
      "                              'VI': {'length': np.float64(54.257),\n",
      "                                     'reward': np.float64(0.523),\n",
      "                                     'time': 0.02655339241027832}},\n",
      " 'ExpandedFrozenLake-v1': {'PI': {'iters': 3,\n",
      "                                  'length': np.float64(6.0),\n",
      "                                  'reward': np.float64(1.0),\n",
      "                                  'time': 0.006121158599853516},\n",
      "                           'VI': {'length': np.float64(6.0),\n",
      "                                  'reward': np.float64(1.0),\n",
      "                                  'time': 0.0007071495056152344}},\n",
      " 'ExpandedFrozenLake-v1-slip': {'PI': {'iters': 4,\n",
      "                                       'length': np.float64(25.178),\n",
      "                                       'reward': np.float64(1.0),\n",
      "                                       'time': 0.045786380767822266},\n",
      "                                'VI': {'length': np.float64(25.636),\n",
      "                                       'reward': np.float64(0.998),\n",
      "                                       'time': 0.014046669006347656}},\n",
      " 'FrozenLake-v1': {'PI': {'iters': 3,\n",
      "                          'length': np.float64(45.481),\n",
      "                          'reward': np.float64(0.713),\n",
      "                          'time': 0.025702476501464844},\n",
      "                   'VI': {'length': np.float64(44.8),\n",
      "                          'reward': np.float64(0.741),\n",
      "                          'time': 0.01922750473022461}}}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for env_id in env_ids:\n",
    "    env = gym.make(env_id)\n",
    "    env_model = env.unwrapped.P  # transition model\n",
    "    nStates, nAction = env.observation_space.n, env.action_space.n\n",
    "\n",
    "    # Value Iteration\n",
    "    t0 = time.time()\n",
    "    policy_VI, values_VI = value_iteration(env_model, nStates, nAction)\n",
    "    t_vi = time.time() - t0\n",
    "    reward_VI, length_VI = evaluate_policy(env, policy_VI)\n",
    "\n",
    "    # Policy Iteration\n",
    "    t0 = time.time()\n",
    "    policy_PI, values_PI, iters = policy_iteration(env_model, nStates, nAction)\n",
    "    t_pi = time.time() - t0\n",
    "    reward_PI, length_PI = evaluate_policy(env, policy_PI)\n",
    "\n",
    "    results[env_id] = {\n",
    "        'VI': {'time': t_vi, 'reward': reward_VI, 'length': length_VI},\n",
    "        'PI': {'time': t_pi, 'reward': reward_PI, 'length': length_PI, 'iters': iters}\n",
    "    }\n",
    "\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597eb5e8",
   "metadata": {},
   "source": [
    "# Results Summary\n",
    "\n",
    "Here's a detailed overview of the results comparing Policy Iteration (PI) and Value Iteration (VI) across different FrozenLake environments:\n",
    "\n",
    "## Performance Comparison\n",
    "\n",
    "| Environment | Algorithm | Iterations | Time (s) | Avg. Reward | Avg. Length |\n",
    "|-------------|-----------|------------|----------|-------------|-------------|\n",
    "| **CustomFrozenLake-v1**<br>(5×5, no slip) | PI | 3 | 0.0032 | 1.0 | 8.0 |\n",
    "| | VI | - | 0.0007 | 1.0 | 8.0 |\n",
    "| **CustomFrozenLake-v1-slip**<br>(5×5, slippery) | PI | 3 | 0.0460 | 0.576 | 53.7 |\n",
    "| | VI | - | 0.0266 | 0.523 | 54.3 |\n",
    "| **ExpandedFrozenLake-v1**<br>(4×4 + key, no slip) | PI | 3 | 0.0061 | 1.0 | 6.0 |\n",
    "| | VI | - | 0.0007 | 1.0 | 6.0 |\n",
    "| **ExpandedFrozenLake-v1-slip**<br>(4×4 + key, slippery) | PI | 4 | 0.0458 | 1.0 | 25.2 |\n",
    "| | VI | - | 0.0140 | 0.998 | 25.6 |\n",
    "| **FrozenLake-v1**<br>(Default 4×4) | PI | 3 | 0.0257 | 0.713 | 45.5 |\n",
    "| | VI | - | 0.0192 | 0.741 | 44.8 |\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "1. **Perfect Performance in Non-Slippery Environments**:\n",
    "   - Both algorithms achieved 100% success rate (reward=1.0) in all non-slippery environments\n",
    "   - VI consistently executed faster (3-10× speedup)\n",
    "\n",
    "2. **Slippery Environment Performance**:\n",
    "   - Expanded environment maintained near-perfect performance (99.8-100%)\n",
    "   - Custom environment showed moderate success (52-58%)\n",
    "   - Original FrozenLake had 71-74% success rate\n",
    "\n",
    "3. **Path Efficiency**:\n",
    "   - Non-slippery environments had optimal paths (6-8 steps)\n",
    "   - Expanded environment maintained relatively efficient paths (~25 steps) even when slippery\n",
    "   - Original FrozenLake had longest paths (~45 steps)\n",
    "\n",
    "4. **Algorithm Comparison**:\n",
    "   - VI consistently executed faster than PI (30-70% reduction)\n",
    "   - Both algorithms converged quickly (3-4 iterations)\n",
    "   - Performance differences were minimal in all the environments\n",
    "\n",
    "5. **Environment Complexity**:\n",
    "   - Key mechanics in expanded environment were successfully navigated\n",
    "   - Larger 5×5 grid showed more performance degradation when slippery\n",
    "   - Original FrozenLake proved most challenging despite smaller size (on the basis of average length)\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The DP algorithms demonstrated awesome performance in deterministic environments, with VI being computationally faster. In stochastic environments, performance varied based on map design, with the expanded key-based environment showing very less drop in reward to slippage. The original FrozenLake environment required the most length on average and thus proved more challenging for the agent despite being smaller."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
