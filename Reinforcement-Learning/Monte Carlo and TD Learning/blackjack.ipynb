{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8473cf05",
   "metadata": {},
   "source": [
    "# BlackJack - using RL ðŸ‚¿ ðŸ‚¡ ðŸ‚¾ ðŸ‚­ ðŸƒ› ðŸ‚± ðŸ‚¿\n",
    "\n",
    "> We will be finding the most optimal policy for BlackJack - a popular casino game using reinforcement learning techniques. We will be using Monte Carlo, Q learning\n",
    "and SARSA algorithm and compare the final result to see which is performing the best and draw intuition from the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6c73a7",
   "metadata": {},
   "source": [
    "## Game setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d9d0b",
   "metadata": {},
   "source": [
    "A single round goes on like this :\n",
    "- place your bet\n",
    "- deal cards\n",
    "  - You get two cards face up\n",
    "  - The dealer gets one card face up and one face down\n",
    "- player's turn\n",
    "- dealer's turn\n",
    "- Win/Lose\n",
    "  - Beat the dealer without busting - win a 1:1 payout\n",
    "  - You get BlackJack but dealer doesn't - win a 3:2 payout\n",
    "  - If you bust or dealer beats you - lose your bet\n",
    "  - Both get the same number - tie (No profit/loss)\n",
    "\n",
    "\n",
    "### Your Options During Play\n",
    "\n",
    "1)    Hit: Take another card.\n",
    "\n",
    "       - Example: You have 12 (7 + 5). You \"hit\" and get a 9 â†’ 21! You stand.\n",
    "\n",
    "1)    Stand: Keep your current hand.\n",
    "\n",
    "       - Example: You have 18 (10 + 8). You \"stand\" to avoid busting.\n",
    "\n",
    "1)    Double Down: Double your bet, take one more card, then stand.\n",
    "\n",
    "       - Example: You have 11 (5 + 6). You double your bet, take one card (e.g., 10), and now have 21.\n",
    "\n",
    "1)    Split: If your first two cards match (e.g., two 8s), split them into two separate hands (each with its own bet).\n",
    "\n",
    "       - Example: You have two 8s (16). You split â†’ now play two hands: Hand 1 (8 + ?) and Hand 2 (8 + ?).\n",
    "\n",
    "       - Aces: If you split Aces, you usually get only one card per Ace.\n",
    "\n",
    "1)    Surrender (if allowed): Give up your hand and lose half your bet.\n",
    "\n",
    "       - Example: You have 16 (10 + 6), dealer shows a 10. You surrender to save half your bet.\n",
    "\n",
    "1)    Insurance: If the dealerâ€™s upcard is an Ace, you can bet half your original wager that the dealer has Blackjack.\n",
    "\n",
    "       - If dealer has Blackjack, insurance pays 2:1.\n",
    "\n",
    "       - If not, you lose the insurance bet.\n",
    "\n",
    "### Dealer Rules\n",
    "\n",
    "-   The dealer must follow strict rules:\n",
    "\n",
    "    -   Hit until 17 or higher.\n",
    "\n",
    "    -   Stand on 17+."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a7a08",
   "metadata": {},
   "source": [
    "## Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b1c8f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "from typing import Tuple, List\n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "from gymnasium import spaces\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import jdc\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572bf5b3",
   "metadata": {},
   "source": [
    "## Timer decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ff90f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        duration = end - start\n",
    "        if args and hasattr(args[0], '__dict__'):\n",
    "            setattr(args[0], f'{func.__name__}_time', duration)\n",
    "        print(f\"Function '{func.__name__}' took {duration:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d5fa5",
   "metadata": {},
   "source": [
    "## BlackJack Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7852f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from collections import deque\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super(BlackjackEnv, self).__init__()\n",
    "\n",
    "        # Action space: 0=stand, 1=hit, 2=double down, 3=split\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # Observation space\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(32),  # Player total (0-31)\n",
    "            spaces.Discrete(11),  # Dealer upcard (0-10, 0=not visible)\n",
    "            spaces.Discrete(2),   # Usable ace (0=False, 1=True)\n",
    "            spaces.Discrete(2),   # Can split (0=False, 1=True)\n",
    "            spaces.Discrete(2),   # Hand active (0=inactive, 1=active)\n",
    "        ))\n",
    "        \n",
    "        # Game state variables\n",
    "        self.player_hands = None\n",
    "        self.dealer_hand = None\n",
    "        self.current_hand_idx = None\n",
    "        self.done = None\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Initialize game\n",
    "        self.player_hands = [{'cards': [], 'bet': 1, 'active': True}]\n",
    "        self.dealer_hand = []\n",
    "        self.current_hand_idx = 0\n",
    "        self.done = False\n",
    "        \n",
    "        # Deal initial cards\n",
    "        self._deal_initial_cards()\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self.render()\n",
    "        \n",
    "        # Return initial obs\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "            hand = self.player_hands[self.current_hand_idx]\n",
    "            terminated = False\n",
    "            reward = 0\n",
    "            \n",
    "            # lights, camera and ACTION\n",
    "            if action == 0:  # Stand\n",
    "                hand['active'] = False\n",
    "            elif action == 1:  # Hit\n",
    "                self._deal_card(hand['cards'])\n",
    "                if self._hand_value(hand['cards']) > 21:\n",
    "                    hand['active'] = False\n",
    "            elif action == 2:  # Double down\n",
    "                hand['bet'] *= 2\n",
    "                self._deal_card(hand['cards'])\n",
    "                hand['active'] = False\n",
    "            elif action == 3:  # Split\n",
    "                if len(hand['cards']) == 2 and hand['cards'][0] == hand['cards'][1]:\n",
    "                    # Create new hand with same bet\n",
    "                    new_hand = {\n",
    "                        'cards': [hand['cards'].pop()],\n",
    "                        'bet': hand['bet'],\n",
    "                        'active': True\n",
    "                    }\n",
    "                    self.player_hands.append(new_hand)\n",
    "                    # Deal to both hands\n",
    "                    self._deal_card(hand['cards'])\n",
    "                    self._deal_card(new_hand['cards'])\n",
    "                else:\n",
    "                    # Invalid split - we will treat it as stand for ease\n",
    "                    hand['active'] = False\n",
    "\n",
    "            # Check if current hand is done\n",
    "            if not hand['active']:\n",
    "                # Move to next active hand\n",
    "                self._activate_next_hand()\n",
    "            \n",
    "            # payout if all hands are done\n",
    "            if self.done:\n",
    "                reward = self._payout()\n",
    "                terminated = True\n",
    "\n",
    "            return self._get_obs(), reward, terminated, False, {}\n",
    "    \n",
    "    def render(self):\n",
    "            if self.render_mode != 'human':\n",
    "                return\n",
    "                \n",
    "            print(\" *** \")\n",
    "            print(f\"Current Hand: {self.current_hand_idx+1}/{len(self.player_hands)}\")\n",
    "            print(f\"Your Cards: {self.player_hands[self.current_hand_idx]['cards']}\")\n",
    "            print(f\"Your Total: {self._hand_value(self.player_hands[self.current_hand_idx]['cards'])}\")\n",
    "            print(f\"Dealer Shows: {self.dealer_hand[0]}\")\n",
    "            print(f\"Current Bet: {self.player_hands[self.current_hand_idx]['bet']}\")\n",
    "            print(\"0: Stand | 1: Hit | 2: Double Down | 3: Split\")\n",
    "            print(\" *** \")\n",
    "            \n",
    "    def _deal_initial_cards(self):\n",
    "        # Deal to player \n",
    "        self._deal_card(self.player_hands[0]['cards'])\n",
    "        self._deal_card(self.player_hands[0]['cards'])\n",
    "        \n",
    "        # Deal to dealer (one up, one down)\n",
    "        self._deal_card(self.dealer_hand)\n",
    "        self._deal_card(self.dealer_hand)\n",
    "\n",
    "    def _deal_card(self, hand_cards):\n",
    "        card = self.np_random.choice([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10])\n",
    "        hand_cards.append(card)\n",
    "\n",
    "    def _hand_value(self, cards):\n",
    "        total = sum(cards)\n",
    "        aces = cards.count(1) + cards.count(11)\n",
    "        \n",
    "        # Convert aces from 11 to 1 if needed\n",
    "        while total > 21 and aces > 0:\n",
    "            total -= 10\n",
    "            aces -= 1\n",
    "            \n",
    "        return total\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        hand = self.player_hands[self.current_hand_idx]\n",
    "        cards = hand['cards']\n",
    "        \n",
    "        # Calculate hand value and usable ace\n",
    "        total = sum(cards)\n",
    "        usable_ace = 0\n",
    "        if 1 in cards and total <= 11:\n",
    "            total += 10\n",
    "            usable_ace = 1\n",
    "            \n",
    "        # Check split e\n",
    "        can_split = int(\n",
    "            len(cards) == 2 and \n",
    "            cards[0] == cards[1] and \n",
    "            len(self.player_hands) < 4  # Max 4 hands\n",
    "        )\n",
    "        \n",
    "        return (\n",
    "            min(total, 31),  \n",
    "            self.dealer_hand[0],\n",
    "            usable_ace,\n",
    "            can_split,\n",
    "            int(hand['active'])\n",
    "            )\n",
    "    def _activate_next_hand(self):\n",
    "        # Find next active hand\n",
    "        for i in range(self.current_hand_idx + 1, len(self.player_hands)):\n",
    "            if self.player_hands[i]['active']:\n",
    "                self.current_hand_idx = i\n",
    "                return\n",
    "                \n",
    "        # No more active hands\n",
    "        self.done = True\n",
    "\n",
    "    def _payout(self):\n",
    "        total_reward = 0\n",
    "        dealer_value = self._dealer_play()\n",
    "        \n",
    "        for hand in self.player_hands:\n",
    "            player_value = self._hand_value(hand['cards'])\n",
    "            bet = hand['bet']\n",
    "            \n",
    "            # Player busts\n",
    "            if player_value > 21:\n",
    "                total_reward -= bet\n",
    "                continue\n",
    "                \n",
    "            # Dealer busts\n",
    "            if dealer_value > 21:\n",
    "                total_reward += bet\n",
    "                continue\n",
    "                \n",
    "            # Compare values\n",
    "            if player_value > dealer_value:\n",
    "                # Blackjack pays 3:2\n",
    "                if len(hand['cards']) == 2 and player_value == 21:\n",
    "                    total_reward += bet * 1.5\n",
    "                else:\n",
    "                    total_reward += bet\n",
    "            elif player_value < dealer_value:\n",
    "                total_reward -= bet\n",
    "            # Push returns 0\n",
    "            \n",
    "        return total_reward\n",
    "    \n",
    "    def _dealer_play(self):\n",
    "            # Dealer plays according to fixed rules\n",
    "            while True:\n",
    "                value = self._hand_value(self.dealer_hand)\n",
    "                if value >= 17:\n",
    "                    return value\n",
    "                self._deal_card(self.dealer_hand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe27f98",
   "metadata": {},
   "source": [
    "### Registering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "53f39690",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(\n",
    "    id=\"Blackjack-v0\",\n",
    "    entry_point=BlackjackEnv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "65151db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *** \n",
      "Current Hand: 1/1\n",
      "Your Cards: [np.int64(9), np.int64(9)]\n",
      "Your Total: 18\n",
      "Dealer Shows: 1\n",
      "Current Bet: 1\n",
      "0: Stand | 1: Hit | 2: Double Down | 3: Split\n",
      " *** \n",
      " *** \n",
      "Current Hand: 1/1\n",
      "Your Cards: [np.int64(9), np.int64(9)]\n",
      "Your Total: 18\n",
      "Dealer Shows: 1\n",
      "Current Bet: 1\n",
      "0: Stand | 1: Hit | 2: Double Down | 3: Split\n",
      " *** \n",
      " *** \n",
      "Current Hand: 1/1\n",
      "Your Cards: [np.int64(9), np.int64(9), np.int64(1)]\n",
      "Your Total: 19\n",
      "Dealer Shows: 1\n",
      "Current Bet: 1\n",
      "0: Stand | 1: Hit | 2: Double Down | 3: Split\n",
      " *** \n",
      " *** \n",
      "Current Hand: 1/1\n",
      "Your Cards: [np.int64(9), np.int64(9), np.int64(1)]\n",
      "Your Total: 19\n",
      "Dealer Shows: 1\n",
      "Current Bet: 1\n",
      "0: Stand | 1: Hit | 2: Double Down | 3: Split\n",
      " *** \n",
      " *** \n",
      "Current Hand: 1/1\n",
      "Your Cards: [np.int64(9), np.int64(9), np.int64(1)]\n",
      "Your Total: 19\n",
      "Dealer Shows: 1\n",
      "Current Bet: 1\n",
      "0: Stand | 1: Hit | 2: Double Down | 3: Split\n",
      " *** \n",
      " *** \n",
      "Current Hand: 1/1\n",
      "Your Cards: [np.int64(9), np.int64(9), np.int64(1)]\n",
      "Your Total: 19\n",
      "Dealer Shows: 1\n",
      "Current Bet: 1\n",
      "0: Stand | 1: Hit | 2: Double Down | 3: Split\n",
      " *** \n",
      "Final Reward: 0\n"
     ]
    }
   ],
   "source": [
    "env = BlackjackEnv(render_mode='human')\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()  # Replace with agent policy\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    env.render()  # Optional visual output\n",
    "\n",
    "env.render()\n",
    "print(f\"Final Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf72f5",
   "metadata": {},
   "source": [
    "## Monte Carlo Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ae9a96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def monte_carlo(env, episodes=10000, alpha=0.1, discount=0.99, epsilon=0.1):\n",
    "\n",
    "    \"\"\"\n",
    "    Monte Carlo control using first-visit method and epsilon-greedy policy.\n",
    "    Returns Q table of state-action values.\n",
    "    \"\"\"\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    Q = defaultdict(lambda: np.zeros(n_actions))\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode = []\n",
    "\n",
    "        while not done:\n",
    "            # epsilon-greedy on Q[state]\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                vals = Q[tuple(state)]\n",
    "                best_actions = np.flatnonzero(vals == vals.max())\n",
    "                action = int(np.random.choice(best_actions))\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode.append((tuple(state), action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for s, a, r in reversed(episode):\n",
    "            G = discount * G + r\n",
    "            if (s, a) not in visited:\n",
    "                visited.add((s, a))\n",
    "                Q[s][a] += alpha * (G - Q[s][a])\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04f6c2",
   "metadata": {},
   "source": [
    "## Temporal Difference Implementation\n",
    "\n",
    "A generic TD update for $Q(s_t, a_t)$ takes the form:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[\\text{Target} - Q(s_t, a_t)\\right],\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a step-size (learning rate), and Target is an estimate of the return just one-step ahead plus estimated future values.\n",
    "\n",
    "- In **SARSA**, the target is:\n",
    "\n",
    "$$\n",
    "r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}),\n",
    "$$\n",
    "\n",
    "using the next action $a_{t+1}$ actually chosen by the current policy (**on-policy**).\n",
    "\n",
    "- In **Q-learning**, the target is:\n",
    "\n",
    "$$\n",
    "r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a'),\n",
    "$$\n",
    "\n",
    "using the best possible next action according to current $Q$ (**off-policy**, because it imagines following the greedy policy from the next state even if the       behavior policy actually explores).\n",
    "\n",
    "Summing up:\n",
    "\n",
    "- **SARSA update**:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)\\right].\n",
    "$$\n",
    "\n",
    "- **Q-learning update**:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)\\right].\n",
    "$$\n",
    "\n",
    "In both cases, during learning we select actions via an $\\epsilon$-greedy policy over current $Q$: with probability $\\epsilon$ choose a random action, else choose:\n",
    "\n",
    "$$\n",
    "\\arg\\max_a Q(s, a).\n",
    "$$\n",
    "\n",
    "This ensures exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4f0764",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def q_learning(env, episodes=1000, alpha=0.1, discount=0.99, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Q-Learning with epsilon-greedy\n",
    "    \"\"\"\n",
    "    n_actions = env.action_space.n\n",
    "    # Q[state_tuple] -> np.array of action-values\n",
    "    Q = defaultdict(lambda: np.zeros(n_actions, dtype=float))\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = tuple(state)  # convert to hashable\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                vals = Q[state]\n",
    "                best_actions = np.flatnonzero(vals == vals.max())\n",
    "                action = int(np.random.choice(best_actions))\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = tuple(next_state)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Q-Learning update (off-policy)\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + discount * np.max(Q[next_state])\n",
    "            Q[state][action] += alpha * (target - Q[state][action])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68463413",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def sarsa(env, episodes=1000, alpha=0.1, discount=0.99, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    SARSA (on-policy TD) with epsilon-greedy\n",
    "    \"\"\"\n",
    "    n_actions = env.action_space.n\n",
    "    Q = defaultdict(lambda: np.zeros(n_actions, dtype=float))\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = tuple(obs)\n",
    "\n",
    "        # Choose initial action\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            vals = Q[state]\n",
    "            best_actions = np.flatnonzero(vals == vals.max())\n",
    "            action = int(np.random.choice(best_actions))\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = tuple(next_obs)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Choose next action (epsilon-greedy)\n",
    "            if random.random() < epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                vals_next = Q[next_state]\n",
    "                best_actions = np.flatnonzero(vals_next == vals_next.max())\n",
    "                next_action = int(np.random.choice(best_actions))\n",
    "\n",
    "            # SARSA update: if done, we treat Q[next_state, next_action] as 0\n",
    "            target = reward + (discount * Q[next_state][next_action] if not done else 0.0)\n",
    "            Q[state][action] += alpha * (target - Q[state][action])\n",
    "\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899f6b7",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "Testing our policy on the environments and  printing the time for al the algorithms and the average return obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761511db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, Q, episodes=100, discount=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate a given policy by running episodes.\n",
    "    Returns the average total discounted return.\n",
    "    \"\"\"\n",
    "    total_return = 0.0\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        G = 0.0\n",
    "        t = 0\n",
    "        while not done:\n",
    "            # Greedy action\n",
    "            best_actions = np.argwhere(Q[state] == np.max(Q[state])).flatten()\n",
    "            action = int(np.random.choice(best_actions))\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            G += (discount**t) * reward\n",
    "            t += 1 # keeping track of the power to raise discount with\n",
    "            state = next_state\n",
    "        total_return += G\n",
    "    avg_return = total_return / episodes\n",
    "    return avg_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba3eb8",
   "metadata": {},
   "source": [
    "## Testing the Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "679decf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'monte_carlo' took 468.8506 seconds\n",
      "Function 'q_learning' took 510.0618 seconds\n",
      "Function 'sarsa' took 554.9023 seconds\n",
      "Average Return (MC): -0.05497446518940097\n",
      "Average Return (Q-Learning): -0.08789791263351324\n",
      "Average Return (SARSA): -0.07783266020118046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Blackjack-v0\")\n",
    "# Train and evaluate each algorithm\n",
    "mc_Q = monte_carlo(env, episodes=5000000, alpha=0.03, discount=0.99, epsilon=0.05)\n",
    "ql_Q = q_learning(env, episodes=5000000, alpha=0.1, discount=0.99, epsilon=0.05)\n",
    "sa_Q = sarsa(env, episodes=5000000, alpha=0.1, discount=0.99, epsilon=0.05)\n",
    "\n",
    "print(\"Average Return (MC):\", evaluate_policy(env, mc_Q, episodes=1000, discount=0.99))\n",
    "print(\"Average Return (Q-Learning):\", evaluate_policy(env, ql_Q, episodes=1000, discount=0.99))\n",
    "print(\"Average Return (SARSA):\", evaluate_policy(env, sa_Q, episodes=1000, discount=0.99))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a5745",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "Even after 5 million episodes, none of the algorithms average return was positive though it's very close to zero. It might be that the game becomes even when played many times and you are likely to neither gain nor lose anything in a long turn. Of course this is just a speculation. Another possibility might be that our model free algorithms are not suited for this environment and we need to discover more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
