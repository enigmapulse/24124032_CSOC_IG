{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7639a199",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Monte Carlo and TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4d737",
   "metadata": {},
   "source": [
    "> In this notebook, we will be implementing Monte Carlo control and Temporal difference learning algorithms - Q learning and SARSA algorithms. We will be testing our implementation on FrozenLake environment and some custom versions we made. Finally we will compare and try to draw conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5aaf63",
   "metadata": {},
   "source": [
    "## Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "17797843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "from typing import Tuple, List\n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "from gymnasium import spaces\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a086487",
   "metadata": {},
   "source": [
    "## Timer decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8106753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        duration = end - start\n",
    "        if args and hasattr(args[0], '__dict__'):\n",
    "            setattr(args[0], f'{func.__name__}_time', duration)\n",
    "        print(f\"Function '{func.__name__}' took {duration:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfbc3e4",
   "metadata": {},
   "source": [
    "## Cliff Walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4872909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalking:\n",
    "    def __init__(self, rows: int = 4, cols: int = 12):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.start = (rows - 1, 0)\n",
    "        self.goal = (rows - 1, cols - 1)\n",
    "        self.cliff = {(rows-1,c) for c in range(1,cols-1)}\n",
    "\n",
    "        self.actions = {\n",
    "            0: (-1, 0),  # up\n",
    "            1: ( 0, 1),  # right\n",
    "            2: ( 1, 0),  # down\n",
    "            3: ( 0,-1),  # left\n",
    "        }\n",
    "\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_states = rows * cols\n",
    "        self.reset()\n",
    "\n",
    "    def state_to_index(self, pos: Tuple[int,int]) -> int:\n",
    "        r, c = pos\n",
    "        return r * self.cols + c\n",
    "\n",
    "    def index_to_state(self, idx: int) -> Tuple[int,int]:\n",
    "        return divmod(idx, self.cols)\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = self.start\n",
    "        return self.state_to_index(self.agent_pos), {}\n",
    "\n",
    "    def step(self, action: int) -> Tuple[int, float, bool, bool, int]:\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(f\"Invalid action {action}\")\n",
    "\n",
    "        truncated = False\n",
    "\n",
    "        dr, dc = self.actions[action]\n",
    "        r, c = self.agent_pos\n",
    "        new_r = min(max(r + dr, 0), self.rows - 1)\n",
    "        new_c = min(max(c + dc, 0), self.cols - 1)\n",
    "        new_pos = (new_r, new_c)\n",
    "\n",
    "        if new_pos in self.cliff:\n",
    "            reward = -100.0\n",
    "            self.agent_pos = self.start\n",
    "            done = False\n",
    "        elif new_pos == self.goal:\n",
    "            reward = -1.0\n",
    "            self.agent_pos = new_pos\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1.0\n",
    "            self.agent_pos = new_pos\n",
    "            done = False\n",
    "\n",
    "        next_state = self.state_to_index(self.agent_pos)\n",
    "        return next_state, reward, done, truncated, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd582eca",
   "metadata": {},
   "source": [
    "## Monte Carlo Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "665b47bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def monte_carlo(env, episodes=10000, alpha=0.1, discount=0.99, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Monte Carlo control using first-visit method and epsilon-greedy policy.\n",
    "    Returns Q table of state-action values.\n",
    "    \"\"\"\n",
    "    n_actions = env.n_actions\n",
    "    n_states = env.n_states\n",
    "\n",
    "\n",
    "    Q = np.zeros((n_states, n_actions)) #q value function initialization\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode = []\n",
    "\n",
    "        # generating an episode\n",
    "        while not done:\n",
    "            #exploration\n",
    "            if random.random() < epsilon:\n",
    "                action = np.random.choice(n_actions) # choose any action randomly with a probability of epsilon\n",
    "            else :\n",
    "                best_actions = np.argwhere(Q[state] == np.max(Q[state])).flatten() #listof all actions with the max q return\n",
    "                action = int(np.random.choice(best_actions)) #choose any one from them randomly\n",
    "\n",
    "            # perform the chosen action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode.append((state,action,reward))\n",
    "            state = next_state\n",
    "\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in range(len(episode)-1,-1,-1): # compute return backwards\n",
    "            s, a, r = episode[t]\n",
    "            G = discount*G + r # since we are traversing backwards we can discount without keeping track of the number of terms\n",
    "            if (s, a) not in visited:\n",
    "                visited.add((s, a))\n",
    "                # Incremental update\n",
    "                Q[s, a] += alpha * (G - Q[s, a])\n",
    "\n",
    "\n",
    "    return Q\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7ec9f",
   "metadata": {},
   "source": [
    "## Temporal Difference Implementation\n",
    "\n",
    "A generic TD update for $Q(s_t, a_t)$ takes the form:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[\\text{Target} - Q(s_t, a_t)\\right],\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a step-size (learning rate), and Target is an estimate of the return just one-step ahead plus estimated future values.\n",
    "\n",
    "- In **SARSA**, the target is:\n",
    "\n",
    "$$\n",
    "r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}),\n",
    "$$\n",
    "\n",
    "using the next action $a_{t+1}$ actually chosen by the current policy (**on-policy**).\n",
    "\n",
    "- In **Q-learning**, the target is:\n",
    "\n",
    "$$\n",
    "r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a'),\n",
    "$$\n",
    "\n",
    "using the best possible next action according to current $Q$ (**off-policy**, because it imagines following the greedy policy from the next state even if the       behavior policy actually explores).\n",
    "\n",
    "Summing up:\n",
    "\n",
    "- **SARSA update**:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)\\right].\n",
    "$$\n",
    "\n",
    "- **Q-learning update**:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)\\right].\n",
    "$$\n",
    "\n",
    "In both cases, during learning we select actions via an $\\epsilon$-greedy policy over current $Q$: with probability $\\epsilon$ choose a random action, else choose:\n",
    "\n",
    "$$\n",
    "\\arg\\max_a Q(s, a).\n",
    "$$\n",
    "\n",
    "This ensures exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "618e1c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "# -------------------------------------------------\n",
    "# Q_LEARNING (On-Policy Temporal-Difference)\n",
    "# -------------------------------------------------\n",
    "def q_learning(env, episodes=1000, alpha=0.1, discount=0.99, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm with epsilon-greedy exploration.\n",
    "    Returns Q value function\n",
    "    \"\"\"\n",
    "    n_actions = env.n_actions\n",
    "    n_states = env.n_states\n",
    "    Q = np.zeros((n_states, n_actions)) #init q value functions for all pairs\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        \n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = np.random.choice(n_actions)\n",
    "            else:\n",
    "                best_actions = np.argwhere(Q[state] == np.max(Q[state])).flatten()\n",
    "                action = int(np.random.choice(best_actions))\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Q-Learning update (off-policy)\n",
    "            best_next = 0 if done else np.max(Q[next_state]) # choose the best q of the nest state for updating irrespective of our current policy (off policy)\n",
    "            Q[state, action] += alpha * (reward + discount * best_next - Q[state, action])\n",
    "            \n",
    "            state = next_state\n",
    "    return Q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6cef830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "# -------------------------------------------------\n",
    "# SARSA (On-Policy Temporal-Difference)\n",
    "# -------------------------------------------------\n",
    "def sarsa(env, episodes=1000, alpha=0.1, discount=0.99, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    SARSA algorithm (on-policy TD control) with epsilon-greedy policy.\n",
    "    Returns Q table of state-action values.\n",
    "    \"\"\"\n",
    "    n_actions = env.n_actions\n",
    "    n_states = env.n_states\n",
    "    Q = np.zeros((n_states, n_actions)) #init q values for all state action pairs\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        # Choose initial action (epsilon strategy)\n",
    "        if random.random() < epsilon:\n",
    "            action = np.random.choice(n_actions)\n",
    "        else:\n",
    "            best_actions = np.argwhere(Q[state] == np.max(Q[state])).flatten()\n",
    "            action = int(np.random.choice(best_actions))\n",
    "\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Choose next action (epsilon-greedy)\n",
    "            if random.random() < epsilon:\n",
    "                next_action = np.random.choice(n_actions)\n",
    "            else:\n",
    "                best_actions = np.argwhere(Q[next_state] == np.max(Q[next_state])).flatten()\n",
    "                next_action = int(np.random.choice(best_actions))\n",
    "            \n",
    "            # SARSA update (on-policy)\n",
    "            Q[state, action] += alpha * (reward + discount * Q[next_state, next_action] * (not done) - Q[state, action])\n",
    "            state, action = next_state, next_action\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c74d1d5",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "Testing our policy on the environments and  printing the time for al the algorithms and the average return obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e8f8d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def evaluate_policy(env, Q, episodes=100, discount=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate a given policy derived from Q (greedy) by running episodes.\n",
    "    Returns the average total (discounted) return.\n",
    "    \"\"\"\n",
    "    total_return = 0.0\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        G = 0.0\n",
    "        t = 0\n",
    "        while not done:\n",
    "            # Greedy action\n",
    "            best_actions = np.argwhere(Q[state] == np.max(Q[state])).flatten()\n",
    "            action = int(np.random.choice(best_actions))\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            G += (discount**t) * reward\n",
    "            t += 1 # keeping track of the power to raise discount with\n",
    "            state = next_state\n",
    "        total_return += G\n",
    "    avg_return = total_return / episodes\n",
    "    return avg_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e75189",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9f66439b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'monte_carlo' took 31.3385 seconds\n",
      "Function 'q_learning' took 19.7461 seconds\n",
      "Function 'sarsa' took 19.9301 seconds\n",
      "Function 'evaluate_policy' took 0.3606 seconds\n",
      "Average Return (MC): -15.705680661607396\n",
      "Function 'evaluate_policy' took 0.2721 seconds\n",
      "Average Return (Q-Learning): -12.247897700102984\n",
      "Function 'evaluate_policy' took 0.3668 seconds\n",
      "Average Return (SARSA): -15.705680661607396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = CliffWalking()\n",
    "# Train and evaluate each algorithm\n",
    "mc_Q = monte_carlo(env, episodes=50000, alpha=0.03, discount=0.99, epsilon=0.1)\n",
    "ql_Q = q_learning(env, episodes=50000, alpha=0.1, discount=0.99, epsilon=0.1)\n",
    "sa_Q = sarsa(env, episodes=50000, alpha=0.1, discount=0.99, epsilon=0.1)\n",
    "\n",
    "print(\"Average Return (MC):\", evaluate_policy(env, mc_Q, episodes=1000, discount=0.99))\n",
    "print(\"Average Return (Q-Learning):\", evaluate_policy(env, ql_Q, episodes=1000, discount=0.99))\n",
    "print(\"Average Return (SARSA):\", evaluate_policy(env, sa_Q, episodes=1000, discount=0.99))\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
