{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab31069",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Monte Carlo and TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a7f968",
   "metadata": {},
   "source": [
    "> In this notebook, we will be implementing Monte Carlo control and Temporal difference learning algorithms - Q learning and SARSA algorithms. We will be testing our implementation on FrozenLake environment and some custom versions we made. Finally we will compare and try to draw conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c175f3ca",
   "metadata": {},
   "source": [
    "## Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f5e9115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "from gymnasium import spaces\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e691a32",
   "metadata": {},
   "source": [
    "## Timer decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c55548ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        duration = end - start\n",
    "        if args and hasattr(args[0], '__dict__'):\n",
    "            setattr(args[0], f'{func.__name__}_time', duration)\n",
    "        print(f\"Function '{func.__name__}' took {duration:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d85fa80",
   "metadata": {},
   "source": [
    "## Custom Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daabbd71",
   "metadata": {},
   "source": [
    "Here we are creating a new class for our custom environment with a 5*5 custom map and similar methods to the standard environments in the gymnasium package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b046166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frozenlake_desc(size, hole_prob=0.2, seed=None):\n",
    "    import random\n",
    "    rng = random.Random(seed)\n",
    "    # Initialize grid as frozen\n",
    "    desc = [['F'] * size for _ in range(size)]\n",
    "    # Set start and goal\n",
    "    desc[0][0] = 'S'\n",
    "    desc[size-1][size-1] = 'G'\n",
    "    \n",
    "    # Generate a monotonic path (right/down moves only)\n",
    "    path = [(0, 0)]\n",
    "    r, c = 0, 0\n",
    "    while (r, c) != (size-1, size-1):\n",
    "        choices = []\n",
    "        if r < size-1:\n",
    "            choices.append((r+1, c))\n",
    "        if c < size-1:\n",
    "            choices.append((r, c+1))\n",
    "        r, c = rng.choice(choices)\n",
    "        path.append((r, c))\n",
    "    \n",
    "    # Set holes for non-path cells (start/goal skipped automatically)\n",
    "    path_set = set(path)\n",
    "    for r in range(size):\n",
    "        for c in range(size):\n",
    "            if (r, c) not in path_set and rng.random() < hole_prob:\n",
    "                desc[r][c] = 'H'  # Only set holes; 'F' is already default\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "192bf354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFrozenLakeEnv(gym.Env):\n",
    "    def __init__(self, slippery=False):\n",
    "        self.size = 5\n",
    "        self.n_states = self.size * self.size\n",
    "        self.n_actions = 4\n",
    "        self.slippery = slippery\n",
    "        \n",
    "        # Define custom 50x50 map\n",
    "        self.desc = np.array(generate_frozenlake_desc(50, hole_prob=0.2, seed=42), dtype='<U1')\n",
    "        \n",
    "        # Calculate positions\n",
    "        self.start_pos = None\n",
    "        self.goal_pos = []\n",
    "        self.hole_pos = []\n",
    "        for row in range(self.size):  ## idea was that there may be many goals but not doing that anymore\n",
    "            for col in range(self.size):\n",
    "                if self.desc[row, col] == 'S':\n",
    "                    self.start_pos = (row, col)\n",
    "                elif self.desc[row, col] == 'G':\n",
    "                    self.goal_pos.append((row, col))\n",
    "                elif self.desc[row, col] == 'H':\n",
    "                    self.hole_pos.append((row, col))\n",
    "        \n",
    "        # first is row and second is coulumn\n",
    "        self.actions = {\n",
    "            0: (0, -1),    # Left\n",
    "            1: (1, 0),     # Down\n",
    "            2: (0, 1),     # Right\n",
    "            3: (-1, 0)     # Up\n",
    "        }\n",
    "        \n",
    "        # Build transition model\n",
    "        self.P = self._make_transition_model()\n",
    "        \n",
    "        # Define spaces and state\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        self.observation_space = spaces.Discrete(self.n_states)\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self, seed=None, **kwargs):\n",
    "        super().reset(seed=seed, **kwargs)\n",
    "        self.state = self.pos_to_state(self.start_pos) #what;s the satte of the start position\n",
    "        return self.state, {} # we ain't sending any info\n",
    "\n",
    "    def step(self, action):\n",
    "        current_pos = self.state_to_pos(self.state) # we are only storing state not position so this roundabout\n",
    "        row, col = current_pos\n",
    "        \n",
    "        # if it is slippery then some stochastic flavour\n",
    "        if self.slippery:\n",
    "            action = self.np_random.choice([action, (action + 1) % 4, (action - 1) % 4])\n",
    "        \n",
    "\n",
    "        # calculus :)\n",
    "        dr, dc = self.actions[action]\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "        \n",
    "        # Ensure within bounds\n",
    "        new_row = np.clip(new_row, 0, self.size - 1)\n",
    "        new_col = np.clip(new_col, 0, self.size - 1)\n",
    "        new_pos = (new_row, new_col)\n",
    "        new_state = self.pos_to_state(new_pos)\n",
    "        \n",
    "        # Check for hole or goal\n",
    "        terminated = False\n",
    "        reward = 0.0\n",
    "        if new_pos in self.hole_pos:\n",
    "            terminated = True\n",
    "        elif new_pos in self.goal_pos:\n",
    "            terminated = True\n",
    "            reward = 1.0\n",
    "        \n",
    "        self.state = new_state #change state\n",
    "        return new_state, reward, terminated, False, {}\n",
    "\n",
    "    \n",
    "\n",
    "    def _make_transition_model(self):\n",
    "        P = {s: {a: [] for a in range(self.n_actions)} for s in range(self.n_states)} #initializing P[s][a] to an empty list\n",
    "        for s in range(self.n_states):\n",
    "            row, col = self.state_to_pos(s)\n",
    "            # Check if current state is terminal\n",
    "            current_pos = (row, col)\n",
    "            if current_pos in self.hole_pos or current_pos in self.goal_pos:\n",
    "                for a in range(self.n_actions):\n",
    "                    # Terminal state: stay indefinitely with 0 reward\n",
    "                    P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                continue  # Skip normal transition logic\n",
    "\n",
    "            # Existing transition logic for non-terminal states\n",
    "            for a in range(self.n_actions):\n",
    "                outcomes = []\n",
    "                actions_to_consider = ([a] if not self.slippery else [a, (a+1)%4, (a-1)%4])\n",
    "                prob = 1.0 if not self.slippery else 1/3 # stochastic case\n",
    "                for a2 in actions_to_consider:\n",
    "                    #compute new state\n",
    "                    dr, dc = self.actions[a2]\n",
    "                    nr = np.clip(row + dr, 0, self.size - 1)\n",
    "                    nc = np.clip(col + dc, 0, self.size - 1)\n",
    "                    new_pos = (nr, nc)\n",
    "                    ns = self.pos_to_state(new_pos)\n",
    "                    #check and assign prob accordingly\n",
    "                    done = new_pos in self.hole_pos or new_pos in self.goal_pos\n",
    "                    reward = 1.0 if new_pos in self.goal_pos else 0.0\n",
    "                    outcomes.append((prob, ns, reward, done))\n",
    "                P[s][a] = outcomes\n",
    "        return P\n",
    "\n",
    "    # easy peasy\n",
    "    def pos_to_state(self, pos):\n",
    "        row, col = pos\n",
    "        return row * self.size + col\n",
    "\n",
    "    def state_to_pos(self, state):\n",
    "        row = state // self.size\n",
    "        col = state % self.size\n",
    "        return (row, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d3f1d",
   "metadata": {},
   "source": [
    "This is another custom environment similar to FrozenLake with a 4*4 size except with a twist. The agent will be rewarded only when it collects key along its path to reach the goal. This forces the agent to adopt a particular route. Kind of like travelling in a traffic where you are required to visit a stop (say a petrol pump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be057385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedFrozenLakeEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, slippery=False):\n",
    "        self.size = 4\n",
    "        self.n_states_base = self.size * self.size\n",
    "        self.n_states = self.n_states_base * 2  # Double for key status\n",
    "        self.n_actions = 4\n",
    "        self.slippery = slippery\n",
    "        \n",
    "        # Define 4x4 map with key\n",
    "        self.desc = np.array([\n",
    "            ['S', 'F', 'F', 'F'],\n",
    "            ['F', 'H', 'F', 'K'],\n",
    "            ['F', 'F', 'F', 'F'],\n",
    "            ['H', 'F', 'F', 'G']\n",
    "        ], dtype='<U1')\n",
    "        \n",
    "        # Identify key, start, goal, and hole positions\n",
    "        self.start_pos = None\n",
    "        self.goal_pos = []\n",
    "        self.hole_pos = []\n",
    "        self.key_pos = None\n",
    "        for row in range(self.size): # same idea as for the above class\n",
    "            for col in range(self.size):\n",
    "                if self.desc[row, col] == 'S':\n",
    "                    self.start_pos = (row, col)\n",
    "                elif self.desc[row, col] == 'G':\n",
    "                    self.goal_pos.append((row, col))\n",
    "                elif self.desc[row, col] == 'H':\n",
    "                    self.hole_pos.append((row, col))\n",
    "                elif self.desc[row, col] == 'K':\n",
    "                    self.key_pos = (row, col)\n",
    "        \n",
    "        self.actions = {\n",
    "            0: (0, -1),   # Left\n",
    "            1: (1, 0),    # Down\n",
    "            2: (0, 1),    # Right\n",
    "            3: (-1, 0)    # Up\n",
    "        }\n",
    "\n",
    "        self.P = self._make_transition_model()\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        self.observation_space = spaces.Discrete(self.n_states)\n",
    "        # Initialize state\n",
    "        self.state = None\n",
    "        self.has_key = None\n",
    "\n",
    "    def reset(self, seed=None, **kwargs):\n",
    "        super().reset(seed=seed, **kwargs)\n",
    "        self.has_key = False\n",
    "        self.state = self.pos_to_state(self.start_pos)\n",
    "        return self.get_full_state(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        current_pos = self.state_to_pos(self.state)\n",
    "        row, col = current_pos\n",
    "        \n",
    "        # act \n",
    "        if self.slippery:\n",
    "            action = self.np_random.choice([action, (action + 1) % 4, (action - 1) % 4])\n",
    "        \n",
    "        dr, dc = self.actions[action]\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "        \n",
    "        # Ensure within bounds\n",
    "        new_row = np.clip(new_row, 0, self.size - 1)\n",
    "        new_col = np.clip(new_col, 0, self.size - 1)\n",
    "        new_pos = (new_row, new_col)\n",
    "        new_state = self.pos_to_state(new_pos)\n",
    "        \n",
    "        # Check if key is collected\n",
    "        if new_pos == self.key_pos:\n",
    "            self.has_key = True\n",
    "        \n",
    "        # Check for hole or goal\n",
    "        terminated = False\n",
    "        reward = 0.0\n",
    "        if new_pos in self.hole_pos:\n",
    "            terminated = True\n",
    "        elif new_pos in self.goal_pos and self.has_key:\n",
    "            terminated = True\n",
    "            reward = 1.0\n",
    "        \n",
    "        self.state = new_state\n",
    "        full_state = self.get_full_state()\n",
    "        return full_state, reward, terminated, False, {}\n",
    "\n",
    "    def _make_transition_model(self):\n",
    "        n_base = self.n_states_base  # 16 for 4x4 grid\n",
    "        P = {s: {a: [] for a in range(self.n_actions)} for s in range(self.n_states)} #initialize P[s][a] to an empty list\n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            base_state = s % n_base\n",
    "            has_key = (s >= n_base)\n",
    "            row, col = self.state_to_pos(base_state)\n",
    "            current_pos = (row, col)\n",
    "\n",
    "            # Terminal states: hole OR goal with key\n",
    "            if current_pos in self.hole_pos or (current_pos in self.goal_pos and has_key):\n",
    "                for a in range(self.n_actions):\n",
    "                    P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                continue #skipping normal logic for now\n",
    "\n",
    "            for a in range(self.n_actions):\n",
    "                outcomes = []\n",
    "                actions_to_consider = [a] if not self.slippery else [a, (a+1)%4, (a-1)%4]\n",
    "                prob = 1.0 if not self.slippery else 1.0/len(actions_to_consider)\n",
    "                \n",
    "                for a2 in actions_to_consider:\n",
    "                    #compute new state\n",
    "                    dr, dc = self.actions[a2]\n",
    "                    nr = np.clip(row + dr, 0, self.size-1)\n",
    "                    nc = np.clip(col + dc, 0, self.size-1)\n",
    "                    new_pos = (nr, nc)\n",
    "                    new_base_state = self.pos_to_state(new_pos)\n",
    "                    \n",
    "                    # Update key status\n",
    "                    new_has_key = has_key\n",
    "                    if not has_key and new_pos == self.key_pos:\n",
    "                        new_has_key = True\n",
    "                    \n",
    "                    next_state = new_base_state + (n_base * int(new_has_key))\n",
    "                    \n",
    "                    # Check terminal conditions\n",
    "                    terminated = False\n",
    "                    reward = 0.0\n",
    "                    if new_pos in self.hole_pos:\n",
    "                        terminated = True\n",
    "                    elif new_pos in self.goal_pos and new_has_key:\n",
    "                        terminated = True\n",
    "                        reward = 1.0\n",
    "                    \n",
    "                    outcomes.append((prob, next_state, reward, terminated))\n",
    "                \n",
    "                P[s][a] = outcomes\n",
    "        \n",
    "        return P\n",
    "\n",
    "    #trivial stuff\n",
    "    def pos_to_state(self, pos):\n",
    "        row, col = pos\n",
    "        return row * self.size + col\n",
    "\n",
    "    def state_to_pos(self, state):\n",
    "        row = state // self.size\n",
    "        col = state % self.size\n",
    "        return (row, col)\n",
    "\n",
    "    def get_full_state(self):\n",
    "        return self.state + (self.n_states_base * int(self.has_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721ddd7",
   "metadata": {},
   "source": [
    "### Registering these custom environmnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "203ea210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register environments\n",
    "gym.register(\n",
    "    id=\"CustomFrozenLake-v1\",\n",
    "    entry_point=CustomFrozenLakeEnv,\n",
    "    kwargs={'slippery': False},\n",
    ")\n",
    "\n",
    "gym.register(\n",
    "    id=\"ExpandedFrozenLake-v1\",\n",
    "    entry_point=ExpandedFrozenLakeEnv,\n",
    "    kwargs={'slippery': False},\n",
    ")\n",
    "\n",
    "gym.register(\n",
    "    id=\"CustomFrozenLake-v1-slip\",\n",
    "    entry_point=CustomFrozenLakeEnv,\n",
    "    kwargs={'slippery': True},\n",
    ")\n",
    "\n",
    "gym.register(\n",
    "    id=\"ExpandedFrozenLake-v1-slip\",\n",
    "    entry_point=ExpandedFrozenLakeEnv,\n",
    "    kwargs={'slippery': True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ba2b6",
   "metadata": {},
   "source": [
    "We will iterate over these environment ids to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "377b60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env ids to test\n",
    "env_ids = ['FrozenLake-v1','CustomFrozenLake-v1','ExpandedFrozenLake-v1','CustomFrozenLake-v1-slip','ExpandedFrozenLake-v1-slip']  # frozenlake-v1 is for the default "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95544a36",
   "metadata": {},
   "source": [
    "## Monte Carlo Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3dace195",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def monte_carlo(env, episodes=10000, alpha=0.1, discount=0.99, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Monte Carlo control using first-visit method and epsilon-greedy policy.\n",
    "    Returns Q table of state-action values.\n",
    "    \"\"\"\n",
    "    n_actions = env.action_space.n\n",
    "    n_states = env.observation_space.n\n",
    "\n",
    "    \n",
    "\n",
    "    Q = np.zeros((n_states, n_actions)) #q value function initialization\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode = []\n",
    "\n",
    "        # generating an episode\n",
    "        while not done:\n",
    "            #exploration\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample() # choose any action randomly with a probability of epsilon\n",
    "            else :\n",
    "                best_actions = np.argwhere(Q[state] == np.max(Q[state])).flatten() #listof all actions with the max q return\n",
    "                action = int(np.random.choice(best_actions)) #choose any one from them randomly\n",
    "\n",
    "            # perform the chosen action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode.append((state,action,reward))\n",
    "            state = next_state\n",
    "\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in range(len(episode)-1,-1,-1): # compute return backwards\n",
    "            s, a, r = episode[t]\n",
    "            G = discount*G + r # since we are traversing backwards we can discount without keeping track of the number of terms\n",
    "            if (s, a) not in visited:\n",
    "                visited.add((s, a))\n",
    "                # Incremental update\n",
    "                Q[s, a] += alpha * (G - Q[s, a])\n",
    "\n",
    "\n",
    "    return Q\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55abc064",
   "metadata": {},
   "source": [
    "## Temporal Difference Implementation\n",
    "\n",
    "A generic TD update for $Q(s_t, a_t)$ takes the form:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[\\text{Target} - Q(s_t, a_t)\\right],\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a step-size (learning rate), and Target is an estimate of the return just one-step ahead plus estimated future values.\n",
    "\n",
    "- In **SARSA**, the target is:\n",
    "\n",
    "$$\n",
    "r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}),\n",
    "$$\n",
    "\n",
    "using the next action $a_{t+1}$ actually chosen by the current policy (**on-policy**).\n",
    "\n",
    "- In **Q-learning**, the target is:\n",
    "\n",
    "$$\n",
    "r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a'),\n",
    "$$\n",
    "\n",
    "using the best possible next action according to current $Q$ (**off-policy**, because it imagines following the greedy policy from the next state even if the       behavior policy actually explores).\n",
    "\n",
    "Summing up:\n",
    "\n",
    "- **SARSA update**:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)\\right].\n",
    "$$\n",
    "\n",
    "- **Q-learning update**:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)\\right].\n",
    "$$\n",
    "\n",
    "In both cases, during learning we select actions via an $\\epsilon$-greedy policy over current $Q$: with probability $\\epsilon$ choose a random action, else choose:\n",
    "\n",
    "$$\n",
    "\\arg\\max_a Q(s, a).\n",
    "$$\n",
    "\n",
    "This ensures exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67bc5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "# -------------------------------------------------\n",
    "# Q_LEARNING (On-Policy Temporal-Difference)\n",
    "# -------------------------------------------------\n",
    "def q_learning(env, episodes=1000, alpha=0.1, discount=0.99, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm with epsilon-greedy exploration.\n",
    "    Returns Q value function\n",
    "    \"\"\"\n",
    "    n_actions = env.action_space.n\n",
    "    n_states = env.observation_space.n\n",
    "    Q = np.zeros((n_states, n_actions)) #init q value functions for all pairs\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        \n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                best_actions = np.argwhere(Q[state] == np.max(Q[state])).flatten()\n",
    "                action = int(np.random.choice(best_actions))\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Q-Learning update (off-policy)\n",
    "            best_next = 0 if done else np.max(Q[next_state]) # choose the best q of the nest state for updating irrespective of our current policy (off policy)\n",
    "            Q[state, action] += alpha * (reward + discount * best_next - Q[state, action])\n",
    "            \n",
    "            state = next_state\n",
    "    return Q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "413d1af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "# -------------------------------------------------\n",
    "# SARSA (On-Policy Temporal-Difference)\n",
    "# -------------------------------------------------\n",
    "def sarsa(env, episodes=1000, alpha=0.1, discount=0.99, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    SARSA algorithm (on-policy TD control) with epsilon-greedy policy.\n",
    "    Returns Q table of state-action values.\n",
    "    \"\"\"\n",
    "    n_actions = env.action_space.n\n",
    "    n_states = env.observation_space.n\n",
    "    Q = np.zeros((n_states, n_actions)) #init q values for all state action pairs\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        # Choose initial action (epsilon strategy)\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            best_actions = np.argwhere(Q[state] == np.max(Q[state])).flatten()\n",
    "            action = int(np.random.choice(best_actions))\n",
    "\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Choose next action (epsilon-greedy)\n",
    "            if random.random() < epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                best_actions = np.argwhere(Q[next_state] == np.max(Q[next_state])).flatten()\n",
    "                next_action = int(np.random.choice(best_actions))\n",
    "            \n",
    "            # SARSA update (on-policy)\n",
    "            Q[state, action] += alpha * (reward + discount * Q[next_state, next_action] * (not done) - Q[state, action])\n",
    "            state, action = next_state, next_action\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb53ffc",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "Testing our policy on the environments and  printing the time for al the algorithms and the average return obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f669ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, Q, episodes=100, discount=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate a given policy derived from Q (greedy) by running episodes.\n",
    "    Returns the average total (discounted) return.\n",
    "    \"\"\"\n",
    "    total_return = 0.0\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        G = 0.0\n",
    "        t = 0\n",
    "        while not done:\n",
    "            # Greedy action\n",
    "            best_actions = np.argwhere(Q[state] == np.max(Q[state])).flatten()\n",
    "            action = int(np.random.choice(best_actions))\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            G += (discount**t) * reward\n",
    "            t += 1 # keeping track of the power to raise discount with\n",
    "            state = next_state\n",
    "        total_return += G\n",
    "    avg_return = total_return / episodes\n",
    "    return avg_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a04d9",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c35fde59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenLake-v1\n",
      "Function 'monte_carlo' took 28.1259 seconds\n",
      "Function 'q_learning' took 41.7209 seconds\n",
      "Function 'sarsa' took 34.3838 seconds\n",
      "Average Return (MC): 0.5296274978890726\n",
      "Average Return (Q-Learning): 0.5266961739987945\n",
      "Average Return (SARSA): 0.5042581437466054\n",
      "\n",
      "CustomFrozenLake-v1\n",
      "Function 'monte_carlo' took 38.8958 seconds\n",
      "Function 'q_learning' took 42.8572 seconds\n",
      "Function 'sarsa' took 40.4023 seconds\n",
      "Average Return (MC): 0.0\n",
      "Average Return (Q-Learning): 0.0\n",
      "Average Return (SARSA): 0.0\n",
      "\n",
      "ExpandedFrozenLake-v1\n",
      "Function 'monte_carlo' took 15.6120 seconds\n",
      "Function 'q_learning' took 11.9723 seconds\n",
      "Function 'sarsa' took 12.7915 seconds\n",
      "Average Return (MC): 0.9509900499000175\n",
      "Average Return (Q-Learning): 0.9509900499000175\n",
      "Average Return (SARSA): 0.9509900499000175\n",
      "\n",
      "CustomFrozenLake-v1-slip\n",
      "Function 'monte_carlo' took 51.3769 seconds\n",
      "Function 'q_learning' took 55.1089 seconds\n",
      "Function 'sarsa' took 51.7960 seconds\n",
      "Average Return (MC): 0.0\n",
      "Average Return (Q-Learning): 0.0\n",
      "Average Return (SARSA): 0.0\n",
      "\n",
      "ExpandedFrozenLake-v1-slip\n",
      "Function 'monte_carlo' took 57.9315 seconds\n",
      "Function 'q_learning' took 56.8326 seconds\n",
      "Function 'sarsa' took 57.8044 seconds\n",
      "Average Return (MC): 0.7365592485104497\n",
      "Average Return (Q-Learning): 0.7896527170637252\n",
      "Average Return (SARSA): 0.7775056581535342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for eid in env_ids:\n",
    "    print(eid)\n",
    "    env = gym.make(eid)\n",
    "    # Train and evaluate each algorithm\n",
    "    mc_Q = monte_carlo(env, episodes=50000, alpha=0.03, discount=0.99, epsilon=0.1)\n",
    "    ql_Q = q_learning(env, episodes=50000, alpha=0.1, discount=0.99, epsilon=0.1)\n",
    "    sa_Q = sarsa(env, episodes=50000, alpha=0.1, discount=0.99, epsilon=0.1)\n",
    "\n",
    "    print(\"Average Return (MC):\", evaluate_policy(env, mc_Q, episodes=1000, discount=0.99))\n",
    "    print(\"Average Return (Q-Learning):\", evaluate_policy(env, ql_Q, episodes=1000, discount=0.99))\n",
    "    print(\"Average Return (SARSA):\", evaluate_policy(env, sa_Q, episodes=1000, discount=0.99))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
