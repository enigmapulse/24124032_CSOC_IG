{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab31069",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Monte Carlo and TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a7f968",
   "metadata": {},
   "source": [
    "> In this notebook, we will be implementing Monte Carlo control and Temporal difference learning algorithms - Q learning and SARSA algorithms. We will be testing our implementation on FrozenLake environment and some custom versions we made. Finally we will compare and try to draw conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c175f3ca",
   "metadata": {},
   "source": [
    "## Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5e9115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "from gymnasium import spaces\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e691a32",
   "metadata": {},
   "source": [
    "## Timer decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55548ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        duration = end - start\n",
    "        if args and hasattr(args[0], '__dict__'):\n",
    "            setattr(args[0], f'{func.__name__}_time', duration)\n",
    "        print(f\"Function '{func.__name__}' took {duration:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d85fa80",
   "metadata": {},
   "source": [
    "## Custom Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daabbd71",
   "metadata": {},
   "source": [
    "> We will be creating a new custom environment based on FrozenLake environment just with some minor tweaks and test our algorithms on these. These class would have similar methods to the standard environments in the gymnasium package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190fb43",
   "metadata": {},
   "source": [
    "For using a custom environment of large size in frozen lake environemnt, we need to generate a map from starting position to end goal to ensure that there is a possible path which can be learned by our agent.\n",
    "We check at each cell, all the possible choices of moving our agent (basically up and down) and then choose randomly from there. This generates a monotonic path and then we later set other cells as holes according to the passed probability. \n",
    "\n",
    "We could have used all four direction when considering choices, essentially making the path non-monotonic. This is so because a random walk in 2D always ensure that you visit any other cell. However, this is not so desirable as this would increase the size of path and it might be that there are too many frozen cells making it easy for our agent to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7106ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frozenlake_desc(size, hole_prob=0.2, seed=None):\n",
    "    rng = random.Random(seed)\n",
    "    desc = [['F'] * size for _ in range(size)]\n",
    "    desc[0][0] = 'S'\n",
    "    desc[size-1][size-1] = 'G'\n",
    "    \n",
    "    # Generate path with 3-cell wide corridor\n",
    "    path = set()\n",
    "    r, c = 0, 0\n",
    "    while (r, c) != (size-1, size-1):\n",
    "        path.add((r, c))\n",
    "        # Add neighboring cells to create width\n",
    "        for dr, dc in [(0,1), (1,0), (0,-1), (-1,0)]:\n",
    "            nr, nc = r+dr, c+dc\n",
    "            if 0 <= nr < size and 0 <= nc < size:\n",
    "                path.add((nr, nc))\n",
    "                \n",
    "        choices = []\n",
    "        if r < size-1:\n",
    "            choices.append((r+1, c))\n",
    "        if c < size-1:\n",
    "            choices.append((r, c+1))\n",
    "        r, c = rng.choice(choices)\n",
    "    \n",
    "    # Set holes only outside corridor\n",
    "    hole_pos = []\n",
    "    for r in range(size):\n",
    "        for c in range(size):\n",
    "            if (r, c) not in path and rng.random() < hole_prob:\n",
    "                desc[r][c] = 'H'\n",
    "                hole_pos.append((r,c))\n",
    "    return desc, hole_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462ad17",
   "metadata": {},
   "source": [
    "Having made a way to generate the desc of our map for the environment easily we will proceed with our custom environment.\n",
    "\n",
    "A typical environment has some essential features -->\n",
    "- An action apce and an observation space : usually done using spaces module from gymnasium\n",
    "- Reset and Step methods : for taking observation, recording rewards and performing further actions\n",
    "- It also should have an attribute P providing the transition probabilities of our model (this is not required here since we are doing model free learning but in general it is included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192bf354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFrozenLakeEnv(gym.Env):\n",
    "    def __init__(self, slippery=False):\n",
    "\n",
    "        # essentials\n",
    "        self.size = 50\n",
    "        self.n_states = self.size * self.size # number of cells\n",
    "        self.n_actions = 4\n",
    "        self.slippery = slippery\n",
    "        self.hole_pos = []\n",
    "        \n",
    "        # Define a custom 50x50 map\n",
    "        dsc, hp = generate_frozenlake_desc(self.size, hole_prob=0.1, seed=42)\n",
    "        self.desc = np.array(dsc, dtype='<U1')\n",
    "        self.hole_pos = hp\n",
    "        self.hole_pos = set(self.hole_pos)\n",
    "        # Calculate positions\n",
    "        self.start_pos = (0,0)\n",
    "        self.goal_pos = (self.size-1, self.size-1)\n",
    "        # first is row and second is coulumn\n",
    "        self.actions = {\n",
    "            0: (0, -1),    # Left\n",
    "            1: (1, 0),     # Down\n",
    "            2: (0, 1),     # Right\n",
    "            3: (-1, 0)     # Up\n",
    "        }\n",
    "        \n",
    "        # Define spaces and state\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        self.observation_space = spaces.Discrete(self.n_states)\n",
    "        self.state = None # this will be storing the current state of the agent\n",
    "\n",
    "    def reset(self, seed=None, **kwargs):\n",
    "        super().reset(seed=seed, **kwargs)\n",
    "        self.state = self.pos_to_state(self.start_pos) #what's the state of the start position\n",
    "        return self.state, {} # we ain't sending any info\n",
    "\n",
    "    def step(self, action):\n",
    "        current_pos = self.state_to_pos(self.state) # we are only storing state not position so this roundabout\n",
    "        row, col = current_pos\n",
    "        \n",
    "        # if it is slippery then some stochastic flavour\n",
    "        if self.slippery:\n",
    "            action = self.np_random.choice([action, (action + 1) % 4, (action - 1) % 4])\n",
    "        \n",
    "\n",
    "        # calculus :)\n",
    "        dr, dc = self.actions[action]\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "        \n",
    "        # Ensure within bounds\n",
    "        new_row = np.clip(new_row, 0, self.size - 1)\n",
    "        new_col = np.clip(new_col, 0, self.size - 1)\n",
    "        new_pos = (new_row, new_col)\n",
    "        new_state = self.pos_to_state(new_pos)\n",
    "\n",
    "        old_dist = abs(current_pos[0]-self.size+1) + abs(current_pos[1]-self.size+1)\n",
    "        new_dist = abs(new_row-self.size+1) + abs(new_col-self.size+1)\n",
    "        \n",
    "        # Check for hole or goal\n",
    "        terminated = False\n",
    "        reward = 0.0\n",
    "        if new_pos in self.hole_pos:\n",
    "            terminated = True\n",
    "            reward = -1.0\n",
    "        elif new_pos == self.goal_pos:\n",
    "            terminated = True\n",
    "            reward = 1.0\n",
    "        \n",
    "        self.state = new_state #change state\n",
    "        return new_state, reward, terminated, False, {}\n",
    "\n",
    "    # easy peasy\n",
    "    def pos_to_state(self, pos):\n",
    "        row, col = pos\n",
    "        return row * self.size + col\n",
    "\n",
    "    def state_to_pos(self, state):\n",
    "        row = state // self.size\n",
    "        col = state % self.size\n",
    "        return (row, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d3f1d",
   "metadata": {},
   "source": [
    "This is another custom environment similar to FrozenLake with a 4*4 size except with a twist. The agent will be rewarded only when it collects key along its path to reach the goal. This forces the agent to adopt a particular route. Kind of like travelling in a traffic where you are required to visit a stop (say a petrol pump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be057385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedFrozenLakeEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, slippery=False):\n",
    "        self.size = 4\n",
    "        self.n_states_base = self.size * self.size\n",
    "        self.n_states = self.n_states_base * 2  # Double for key status\n",
    "        self.n_actions = 4\n",
    "        self.slippery = slippery\n",
    "        \n",
    "        # Define 4x4 map with key\n",
    "        self.desc = np.array([\n",
    "            ['S', 'F', 'F', 'F'],\n",
    "            ['F', 'H', 'F', 'K'],\n",
    "            ['F', 'F', 'F', 'F'],\n",
    "            ['H', 'F', 'F', 'G']\n",
    "        ], dtype='<U1')\n",
    "        \n",
    "        # Identify key, start, goal, and hole positions\n",
    "        self.start_pos = None\n",
    "        self.goal_pos = []\n",
    "        self.hole_pos = []\n",
    "        self.key_pos = None\n",
    "        for row in range(self.size): # same idea as for the above class\n",
    "            for col in range(self.size):\n",
    "                if self.desc[row, col] == 'S':\n",
    "                    self.start_pos = (row, col)\n",
    "                elif self.desc[row, col] == 'G':\n",
    "                    self.goal_pos.append((row, col))\n",
    "                elif self.desc[row, col] == 'H':\n",
    "                    self.hole_pos.append((row, col))\n",
    "                elif self.desc[row, col] == 'K':\n",
    "                    self.key_pos = (row, col)\n",
    "        \n",
    "        self.actions = {\n",
    "            0: (0, -1),   # Left\n",
    "            1: (1, 0),    # Down\n",
    "            2: (0, 1),    # Right\n",
    "            3: (-1, 0)    # Up\n",
    "        }\n",
    "\n",
    "        # self.P = self._make_transition_model()\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        self.observation_space = spaces.Discrete(self.n_states)\n",
    "        # Initialize state\n",
    "        self.state = None\n",
    "        self.has_key = None\n",
    "\n",
    "    def reset(self, seed=None, **kwargs):\n",
    "        super().reset(seed=seed, **kwargs)\n",
    "        self.has_key = False\n",
    "        self.state = self.pos_to_state(self.start_pos)\n",
    "        return self.get_full_state(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        current_pos = self.state_to_pos(self.state)\n",
    "        row, col = current_pos\n",
    "        \n",
    "        # act \n",
    "        if self.slippery:\n",
    "            action = self.np_random.choice([action, (action + 1) % 4, (action - 1) % 4])\n",
    "        \n",
    "        dr, dc = self.actions[action]\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "        \n",
    "        # Ensure within bounds\n",
    "        new_row = np.clip(new_row, 0, self.size - 1)\n",
    "        new_col = np.clip(new_col, 0, self.size - 1)\n",
    "        new_pos = (new_row, new_col)\n",
    "        new_state = self.pos_to_state(new_pos)\n",
    "        \n",
    "        # Check if key is collected\n",
    "        if new_pos == self.key_pos:\n",
    "            self.has_key = True\n",
    "        \n",
    "        # Check for hole or goal\n",
    "        terminated = False\n",
    "        reward = 0.0\n",
    "        if new_pos in self.hole_pos:\n",
    "            terminated = True\n",
    "        elif new_pos in self.goal_pos and self.has_key:\n",
    "            terminated = True\n",
    "            reward = 1.0\n",
    "        \n",
    "        self.state = new_state\n",
    "        full_state = self.get_full_state()\n",
    "        return full_state, reward, terminated, False, {}\n",
    "\n",
    "    #trivial stuff\n",
    "    def pos_to_state(self, pos):\n",
    "        row, col = pos\n",
    "        return row * self.size + col\n",
    "\n",
    "    def state_to_pos(self, state):\n",
    "        row = state // self.size\n",
    "        col = state % self.size\n",
    "        return (row, col)\n",
    "\n",
    "    def get_full_state(self):\n",
    "        return self.state + (self.n_states_base * int(self.has_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721ddd7",
   "metadata": {},
   "source": [
    "### Registering these custom environmnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203ea210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register environments\n",
    "gym.register(\n",
    "    id=\"CustomFrozenLake-v1\",\n",
    "    entry_point=CustomFrozenLakeEnv,\n",
    "    kwargs={'slippery': False},\n",
    ")\n",
    "\n",
    "gym.register(\n",
    "    id=\"ExpandedFrozenLake-v1\",\n",
    "    entry_point=ExpandedFrozenLakeEnv,\n",
    "    kwargs={'slippery': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ba2b6",
   "metadata": {},
   "source": [
    "We will iterate over these environment ids to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "377b60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env ids to test\n",
    "env_ids = ['CustomFrozenLake-v1','FrozenLake-v1','ExpandedFrozenLake-v1']  # frozenlake-v1 is for the default "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95544a36",
   "metadata": {},
   "source": [
    "## Monte Carlo Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98391c",
   "metadata": {},
   "source": [
    "We will be implementing the algorithm of monte carlo control. It's a on-policy algorithm without exploring start using epsilon soft policy.\n",
    "\n",
    "BlackJack game requires exploration as there are certain high reward moves.We thus use a slowly exponential decaying epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dace195",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def monte_carlo(env, episodes=10000, alpha=0.1, discount=0.99, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Monte Carlo control using evvery-visit method and epsilon-greedy policy.\n",
    "    Returns Q table of state-action values.\n",
    "    \"\"\"\n",
    "    n_actions = env.action_space.n\n",
    "    n_states = env.observation_space.n\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 0.05\n",
    "    decay_rate = 0.01\n",
    "    max_steps = 1000 \n",
    "\n",
    "    Q = np.ones((n_states, n_actions)) #q value function initialization\n",
    "    epsilons = np.maximum(epsilon_min, epsilon_start * np.exp(-decay_rate * np.arange(episodes))) #precompute epsilon\n",
    "\n",
    "    for ep in tqdm(range(episodes), desc=\"MC epsiodes progress\"):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode = []\n",
    "        epsilon = epsilons[ep]  # slow exponential decay of epsilon\n",
    "        steps = 0\n",
    "        # generating an episode\n",
    "        while not done and steps < max_steps:\n",
    "            #exploration\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample() # choose any action randomly with a probability of epsilon\n",
    "            else :\n",
    "                # policy improvement\n",
    "                best_actions = np.argwhere(Q[state] == np.max(Q[state])).flatten() # list of all actions with the max q return\n",
    "                action = int(np.random.choice(best_actions)) #choose any one from them randomly\n",
    "\n",
    "            # perform the chosen action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode.append((state,action,reward))\n",
    "            state = next_state\n",
    "            steps+=1\n",
    "            \n",
    "        G = 0\n",
    "        for t in range(len(episode)-1,-1,-1): # compute return backwards\n",
    "            s, a, r = episode[t]\n",
    "            G = discount*G + r # since we are traversing backwards we can discount without keeping track of the number of terms\n",
    "            Q[s, a] += alpha * (G - Q[s, a])\n",
    "                \n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55abc064",
   "metadata": {},
   "source": [
    "## Temporal Difference Implementation\n",
    "\n",
    "A general TD update for $Q(s_t, a_t)$ is of  this form:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[\\text{Target} - Q(s_t, a_t)\\right],\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a step-size (learning rate), and Target is an estimate of the return just one-step ahead plus estimated future values.\n",
    "\n",
    "- In **SARSA**, the target is:\n",
    "\n",
    "$$\n",
    "r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}),\n",
    "$$\n",
    "\n",
    "using the next action $a_{t+1}$ actually chosen by the current policy (**on-policy**).\n",
    "\n",
    "- In **Q-learning**, the target is:\n",
    "\n",
    "$$\n",
    "r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a'),\n",
    "$$\n",
    "\n",
    "using the best possible next action according to current $Q$ (**off-policy**, because it imagines following the greedy policy from the next state even if the       behavior policy actually explores).\n",
    "\n",
    "Summing up:\n",
    "\n",
    "- **SARSA update**:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)\\right].\n",
    "$$\n",
    "\n",
    "- **Q-learning update**:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)\\right].\n",
    "$$\n",
    "\n",
    "In both cases, during learning we select actions via an $\\epsilon$-greedy policy over current $Q$: with probability $\\epsilon$ choose a random action, else choose:\n",
    "\n",
    "$$\n",
    "\\arg\\max_a Q(s, a).\n",
    "$$\n",
    "\n",
    "This ensures exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67bc5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "# -------------------------------------------------\n",
    "# Q_LEARNING\n",
    "# -------------------------------------------------\n",
    "def q_learning(env, episodes=1000, alpha=0.1, discount=0.99, epsilon=0.1, alpha_decay=0.001):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm with epsilon-greedy exploration.\n",
    "    Returns Q value function\n",
    "    \"\"\"\n",
    "    n_actions = env.action_space.n\n",
    "    n_states = env.observation_space.n\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 0.05\n",
    "    decay_rate = 0.9\n",
    "    max_steps = 1000\n",
    "\n",
    "    # Precompute epsilons\n",
    "    epsilons = np.maximum(epsilon_min, epsilon_start * np.exp(-decay_rate * np.arange(episodes)))\n",
    "    \n",
    "    Q = np.ones((n_states, n_actions)) * 5.0 #init q value functions for all pairs\n",
    "\n",
    "    for ep in tqdm(range(episodes), desc=\"Q learning epsiodes progress\"):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        epsilon = epsilons[ep]  # slow exponential decay of epsilon\n",
    "        steps = 0\n",
    "\n",
    "        # alpha decay\n",
    "        alpha_curr = alpha/(1+(alpha_decay*ep))\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                max_val = np.max(Q[state])\n",
    "                action = np.random.choice(np.flatnonzero(Q[state] == max_val))\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Q-Learning update \n",
    "            best_next = 0 if done else np.max(Q[next_state]) # choose the best q of the nest state for updating irrespective of our current policy (off policy)\n",
    "            Q[state, action] += alpha_curr * (reward + discount * best_next - Q[state, action])\n",
    "            \n",
    "            state = next_state\n",
    "            steps+=1\n",
    "    return Q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d1af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "# -------------------------------------------------\n",
    "# SARSA\n",
    "# -------------------------------------------------10\n",
    "def sarsa(env, episodes=1000, alpha=0.1, discount=0.99, epsilon=0.1, alpha_decay=0.001):\n",
    "    \"\"\"\n",
    "    SARSA algorithm (on-policy TD control) with epsilon-greedy policy.\n",
    "    Returns Q table of state-action values.\n",
    "    \"\"\"\n",
    "    n_actions = env.action_space.n\n",
    "    n_states = env.observation_space.n\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 0.05\n",
    "    decay_rate = 0.001 \n",
    "    max_steps = 1000\n",
    "    \n",
    "    # Precompute epsilons\n",
    "    epsilons = np.maximum(epsilon_min, epsilon_start * np.exp(-decay_rate * np.arange(episodes)))\n",
    "    \n",
    "    Q = np.ones((n_states, n_actions)) * 5.0 #init q values for all state action pairs\n",
    "\n",
    "    for ep in tqdm(range(episodes), desc=\"SARSA epsiodes progress\"):\n",
    "        state, _ = env.reset()\n",
    "        epsilon = epsilons[ep]  # slow exponential decay of epsilon\n",
    "\n",
    "        # Choose initial action (epsilon strategy)\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            best_actions = np.argwhere(Q[state] == np.max(Q[state])).flatten()\n",
    "            action = int(np.random.choice(best_actions))\n",
    "\n",
    "        # alpha decay\n",
    "        alpha_curr = alpha/(1+(alpha_decay*ep))\n",
    "\n",
    "        steps = 0\n",
    "        done = False\n",
    "        while not done and steps < max_steps:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Choose next action (epsilon-greedy)\n",
    "            if random.random() < epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                best_actions = np.argwhere(Q[next_state] == np.max(Q[next_state])).flatten()\n",
    "                next_action = int(np.random.choice(best_actions))\n",
    "            \n",
    "            # SARSA update (on-policy)\n",
    "            Q[state, action] += alpha_curr * (reward + discount * Q[next_state, next_action] * (not done) - Q[state, action])\n",
    "            state, action = next_state, next_action\n",
    "            steps += 1\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb53ffc",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "Testing our policy on the environments and  printing the time for al the algorithms and the average return obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f669ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, Q, episodes=100, discount=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate a given policy derived from Q (greedy) by running episodes.\n",
    "    Returns the average total (discounted) return.\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    best_actions_list = [np.argwhere(Q[s] == np.max(Q[s])).flatten() for s in range(Q.shape[0])]   \n",
    "    max_steps = 1000\n",
    "    for ep in tqdm(range(episodes), desc=\"Evaluating\"):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        G = 0.0\n",
    "        t = 0\n",
    "        steps = 0\n",
    "        while not done and steps < max_steps:\n",
    "            # Greedy action\n",
    "            actions = best_actions_list[state]\n",
    "            action = int(np.random.choice(actions))\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            G += (discount**t) * reward\n",
    "            t += 1 # keeping track of the power to raise discount with\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        returns.append(G)\n",
    "\n",
    "    returns_arr = np.array(returns, dtype=float)\n",
    "\n",
    "    # plt.ion()  # interactive mode\n",
    "    # fig, ax = plt.subplots(figsize=(8,6))\n",
    "    # ax.plot(cum_avg, marker='s', markersize=4, markevery=50)\n",
    "    # ax.set(xlabel='Episode', ylabel='Average return till now', title='Convergence of Average Return')\n",
    "    # ax.grid(True)\n",
    "    # plt.draw()\n",
    "    # plt.pause(0.001) \n",
    "\n",
    "    avg_return = returns_arr.mean()\n",
    "    return avg_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a04d9",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "c35fde59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomFrozenLake-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MC epsiodes progress: 100%|██████████| 10000/10000 [00:04<00:00, 2025.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'monte_carlo' took 4.9393 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [00:00<00:00, 3797.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return (MC): 0.8429431933839271\n",
      "moving to Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q learning epsiodes progress: 100%|██████████| 5000/5000 [00:13<00:00, 363.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'q_learning' took 13.7458 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [00:00<00:00, 3800.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return (Q-Learning): 0.8429431933839271\n",
      "moving to SARSA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SARSA epsiodes progress: 100%|██████████| 5000/5000 [00:08<00:00, 581.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'sarsa' took 8.5974 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [00:00<00:00, 3837.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return (SARSA): 0.8429431933839271\n",
      "\n",
      "FrozenLake-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MC epsiodes progress: 100%|██████████| 10000/10000 [00:02<00:00, 4068.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'monte_carlo' took 2.4597 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [00:00<00:00, 2991.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return (MC): 0.38816767607190616\n",
      "moving to Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q learning epsiodes progress: 100%|██████████| 5000/5000 [00:03<00:00, 1601.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'q_learning' took 3.1243 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [00:00<00:00, 1981.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return (Q-Learning): 0.5054036686203724\n",
      "moving to SARSA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SARSA epsiodes progress: 100%|██████████| 5000/5000 [00:02<00:00, 2135.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'sarsa' took 2.3434 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [00:00<00:00, 2412.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return (SARSA): 0.5188396662855793\n",
      "\n",
      "ExpandedFrozenLake-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MC epsiodes progress: 100%|██████████| 10000/10000 [00:01<00:00, 7140.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'monte_carlo' took 1.4024 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [00:00<00:00, 6879.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return (MC): 0.9509900498999998\n",
      "moving to Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q learning epsiodes progress: 100%|██████████| 5000/5000 [00:03<00:00, 1386.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'q_learning' took 3.6078 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [00:00<00:00, 11519.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return (Q-Learning): 0.9509900498999998\n",
      "moving to SARSA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SARSA epsiodes progress: 100%|██████████| 5000/5000 [00:01<00:00, 3043.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'sarsa' took 1.6445 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [00:00<00:00, 11258.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return (SARSA): 0.9509900498999998\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for eid in env_ids:\n",
    "    print(eid)\n",
    "    EPISODES = 5000\n",
    "    # if eid == \"ExpandedFrozenLake-v1\" or eid == \"ExpandedFrozenLake-v1-slip\":\n",
    "    #     EPISODES = 500000\n",
    "    env = gym.make(eid)\n",
    "    \n",
    "    # Train and evaluate each algorithm\n",
    "    mc_Q = monte_carlo(env, episodes=10000, alpha=0.01, discount=0.99)\n",
    "    print(\"Average Return (MC):\", evaluate_policy(env, mc_Q, episodes=1000, discount=0.99))\n",
    "\n",
    "    print(\"moving to Q\")\n",
    "\n",
    "    ql_Q = q_learning(env, episodes=EPISODES, alpha=0.1, discount=0.99)\n",
    "    print(\"Average Return (Q-Learning):\", evaluate_policy(env, ql_Q, episodes=1000, discount=0.99))\n",
    "\n",
    "    print(\"moving to SARSA\")\n",
    "\n",
    "    sa_Q = sarsa(env, episodes=EPISODES, alpha=0.1, discount=0.99)\n",
    "    print(\"Average Return (SARSA):\", evaluate_policy(env, sa_Q, episodes=1000, discount=0.99))\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c400ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving to Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q learning epsiodes progress: 100%|██████████| 500000/500000 [18:28<00:00, 451.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'q_learning' took 1108.2927 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [00:01<00:00, 766.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return (Q-Learning): 0.37723664692350406\n",
      "moving to SARSA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SARSA epsiodes progress: 100%|██████████| 500000/500000 [20:33<00:00, 405.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'sarsa' took 1233.1371 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [00:01<00:00, 785.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return (SARSA): 0.37723664692350406\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CustomFrozenLake-v1\")\n",
    "EPISODES = 500000\n",
    "# Train and evaluate each algorithm\n",
    "# mc_Q = monte_carlo(env, episodes=50000, alpha=0.03, discount=0.99)\n",
    "# print(\"Average Return (MC):\", evaluate_policy(env, mc_Q, episodes=1000, discount=0.99))\n",
    "\n",
    "print(\"moving to Q\")\n",
    "\n",
    "ql_Q = q_learning(env, episodes=EPISODES, alpha=0.1, discount=0.99)\n",
    "print(\"Average Return (Q-Learning):\", evaluate_policy(env, ql_Q, episodes=1000, discount=0.99))\n",
    "\n",
    "print(\"moving to SARSA\")\n",
    "\n",
    "sa_Q = sarsa(env, episodes=EPISODES, alpha=0.1, discount=0.99)\n",
    "print(\"Average Return (SARSA):\", evaluate_policy(env, sa_Q, episodes=1000, discount=0.99))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c411e",
   "metadata": {},
   "source": [
    "Now trying with alpha decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CustomFrozenLake-v1\")\n",
    "EPISODES = 500000\n",
    "# Train and evaluate each algorithm\n",
    "# mc_Q = monte_carlo(env, episodes=50000, alpha=0.03, discount=0.99)\n",
    "# print(\"Average Return (MC):\", evaluate_policy(env, mc_Q, episodes=1000, discount=0.99))\n",
    "\n",
    "print(\"moving to Q\")\n",
    "\n",
    "ql_Q = q_learning(env, episodes=EPISODES, alpha=0.5, discount=0.99)\n",
    "print(\"Average Return (Q-Learning):\", evaluate_policy(env, ql_Q, episodes=1000, discount=0.99))\n",
    "\n",
    "print(\"moving to SARSA\")\n",
    "\n",
    "sa_Q = sarsa(env, episodes=EPISODES, alpha=0.5, discount=0.99)\n",
    "print(\"Average Return (SARSA):\", evaluate_policy(env, sa_Q, episodes=1000, discount=0.99))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d4665",
   "metadata": {},
   "source": [
    "It is still too costly to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f20ac4",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We tested the following algorithms on our custom environment:\n",
    "\n",
    "- Monte Carlo - On Policy - Every visit - epsilon greedy policy\n",
    "- Q learning control\n",
    "- SARSA\n",
    "\n",
    "Here are the results and stat summarized:\n",
    "\n",
    "| Environment            | Algorithm     | Time (seconds) | Average Return |\n",
    "|------------------------|---------------|-----------------|-----------------|\n",
    "| CustomFrozenLake-v1    | Monte Carlo   | 4.9393          | 0.8429          |\n",
    "| CustomFrozenLake-v1    | Q-Learning    | 13.7458         | 0.8429          |\n",
    "| CustomFrozenLake-v1    | SARSA         | 8.5974          | 0.8429          |\n",
    "| FrozenLake-v1          | Monte Carlo   | 2.4597          | 0.3882          |\n",
    "| FrozenLake-v1          | Q-Learning    | 3.1243          | 0.5054          |\n",
    "| FrozenLake-v1          | SARSA         | 2.3434          | 0.5188          |\n",
    "| ExpandedFrozenLake-v1  | Monte Carlo   | 1.4024          | 0.9510          |\n",
    "| ExpandedFrozenLake-v1  | Q-Learning    | 3.6078          | 0.9510          |\n",
    "| ExpandedFrozenLake-v1  | SARSA         | 1.6445          | 0.9510          |\n",
    "\n",
    "\n",
    "I did try building larger frozen lake environment but with lower number of episodes and max step length the average return was 0. Increasing the number of episodes and max steps could have fized the issue bu the computation was too slow (taking half an hour for a single algorithm) I did try utilizing as many numpy operations as I could but the time gained was not sufficient. I also tried implementing the distance reward saying that if the euclidean distace between the agent and the goal reduced then we would award it a small value and punish it for going away. UNfortunately this did not work as the agent was repeatedly moving between two cells close by to the goal gaining reward without actually reaching the goal. I have also ensured exploration using exponential decay. Every visit MC turned out to be faster than Once visit and converged in fewer episodes, However the compute time per episode also increased. \n",
    "\n",
    "For a 50*50 frozen lake environment\n",
    "\n",
    "Q learning - 1108.3 seconds  - 0.38 <br>\n",
    "SARSA      - 1233.14 seconds - 0.38\n",
    "\n",
    "Monte Carlo is too slow here. Even here with 500,000 episodes the final reward is pretty low compared to other environments. This indicates that the number of episodes required is much higher and thus the compute time would also not be sufficient.\n",
    "\n",
    "Our algorithms also perform much worse on the original frozen lake. This is due to the stochastic nature of the environment caused to slippery nature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
